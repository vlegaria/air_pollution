{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gKXm0WfsNDg",
        "outputId": "2b2d61f0-8257-43e0-fff1-ad1c3c25b51f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GW6CJkfV_pqn",
        "outputId": "6e2b4543-33f8-4bc1-9ca2-794ccc8f0817"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.2.2\n"
          ]
        }
      ],
      "source": [
        "import sklearn\n",
        "print(sklearn.__version__)\n",
        "# Debe ser version 1.2.2 de lo contrario ejecutar:\n",
        "#!pip install scikit-learn==1.2.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kChVpEWGwc5Z"
      },
      "outputs": [],
      "source": [
        "def metrics(y_test, predicciones):\n",
        "\n",
        "    # Coeficiente de determinación (R-cuadrado)\n",
        "    r2 = r2_score(y_test, predicciones)\n",
        "    # Error cuadrado medio\n",
        "    rmse = mean_squared_error(y_true  = y_test, y_pred  = predicciones, squared = False)\n",
        "    # Error Absoluto Medio\n",
        "    mae = mean_absolute_error(y_test, predicciones)\n",
        "    # Error Porcentual Absoluto Medio\n",
        "    # mape = np.mean(np.abs((y_test - predicciones) / y_test)) * 100\n",
        "    epsilon=1e-10\n",
        "    mape = np.mean(np.abs((y_test - predicciones) / (y_test + epsilon))) * 100\n",
        "\n",
        "    print(\"R^2:\", r2)\n",
        "    print(\"RMSE\", rmse)\n",
        "    print(\"MAE:\", mae)\n",
        "    print(\"MAPE:\", mape, \"%\")\n",
        "\n",
        "    r2 = round(r2, 6)\n",
        "    rmse = round(rmse, 6)\n",
        "    mae = round(mae, 6)\n",
        "    mape = round(mape, 6)\n",
        "    return r2, rmse, mae, mape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e5kzJ1JeUTA"
      },
      "source": [
        "# Todos los modelos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99SMQHaReWJo"
      },
      "outputs": [],
      "source": [
        "import openpyxl\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from math import sqrt\n",
        "import matplotlib.dates as mdates\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import r2_score\n",
        "from joblib import dump\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import make_scorer, mean_squared_error, r2_score\n",
        "\n",
        "# RNN\n",
        "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense\n",
        "from sklearn.metrics import make_scorer, mean_squared_error\n",
        "from sklearn.base import BaseEstimator, RegressorMixin\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "import pickle\n",
        "import pytz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener la hora actual en UTC\n",
        "utc_now = datetime.now(pytz.utc)\n",
        "mexico_city_tz = pytz.timezone('America/Mexico_City')\n",
        "mexico_city_now = utc_now.astimezone(mexico_city_tz)\n",
        "hora_formateada = mexico_city_now.strftime('%Y-%m-%d %H:%M')\n",
        "print(\"Hora actual en Ciudad de México:\", hora_formateada)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiJS5xx46Pa6",
        "outputId": "75efac1a-92a7-4305-f049-b68c9647354d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hora actual en Ciudad de México: 2024-06-06 22:17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXQazGjzhmBS"
      },
      "outputs": [],
      "source": [
        "def leer_archivo(station, target):\n",
        "\n",
        "  #file = files[3]\n",
        "  file_path = os.path.join(dir,station)\n",
        "  df  = pd.read_csv(file_path)\n",
        "  dates = df.date\n",
        "  y = df[target]\n",
        "  X = df.drop(columns=['date', 'year', 'month', 'day', 'hour', 'minute'])\n",
        "  X = X.drop(columns=[target])\n",
        "\n",
        "  return X, y, df, file_path\n",
        "\n",
        "\n",
        "# Convertir los datos en secuencias adecuadas para RNN\n",
        "def create_sequences(X, y, time_steps, time_future):\n",
        "    Xs, ys = [], []\n",
        "    for i in range(len(X) - time_steps):\n",
        "        Xs.append(X[i:(i + time_steps)])\n",
        "        ys.append(y[i + time_steps+ time_future])\n",
        "    return np.array(Xs), np.array(ys)\n",
        "\n",
        "\n",
        "def create_sequences2(X, y, time_steps, time_future):\n",
        "  Xs, ys = [], []\n",
        "  for i in range(len(X) - time_steps-time_future):\n",
        "    df = X[i:(i + time_steps)]\n",
        "    array = df.to_numpy()\n",
        "    # Aplanar el array a un vector\n",
        "    vector = array.flatten()\n",
        "    Xs.append(vector)\n",
        "    ys.append(y[i + time_steps+time_future])\n",
        "  return np.array(Xs), np.array(ys)\n",
        "\n",
        "def MLP_grid(X,y, station_name, fecha, best_model_dir, best_param_dir, time_steps, time_future, results_dir):\n",
        "\n",
        "  X_seq, y_seq = create_sequences2(X, y, time_steps, time_future)\n",
        "  # Dividir los datos en conjunto de entrenamiento y prueba\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)\n",
        "  # Dividir los datos en conjunto de entrenamiento y prueba\n",
        "  #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "  # Definir el regresor MLP\n",
        "  #mlp = MLPRegressor(max_iter=1000)\n",
        "  mlp = MLPRegressor(max_iter=1000)\n",
        "\n",
        "  # Definir el diccionario de parámetros para GridSearchCV\n",
        "  param_grid = {\n",
        "      'hidden_layer_sizes': [(10,), (50,), (100,), (50, 50), (100,100), (50,50,50), (100,100,100), (75,75,75,75), (25,25,25,25), (100,150,100,25)],\n",
        "      'activation': ['relu', 'tanh'],\n",
        "      'solver': ['adam', 'sgd'],\n",
        "      'alpha': [0.0001, 0.05],\n",
        "      'learning_rate': ['constant', 'adaptive'],\n",
        "      'max_iter': [200, 500, 1000, 1500],  # Especificar diferentes números de iteraciones\n",
        "  }\n",
        "\n",
        "  # Configurar GridSearchCV\n",
        "\n",
        "  #'neg_mean_squared_error'\n",
        "  grid_search = GridSearchCV(mlp, param_grid, scoring='r2', cv=5, n_jobs=-1, verbose=1 ,return_train_score=True)\n",
        "  # Ajustar GridSearchCV a los datos de entrenamiento\n",
        "  grid_search.fit(X_train, y_train)\n",
        "\n",
        "  # Mejores parámetros encontrados\n",
        "  print(\"Mejores parámetros:\", grid_search.best_params_)\n",
        "\n",
        "  # Obtener el mejor modelo entrenado\n",
        "  best_model = grid_search.best_estimator_\n",
        "\n",
        "  # Predecir en el conjunto de prueba\n",
        "  y_pred = best_model.predict(X_test)\n",
        "\n",
        "  # Calcular el RMSE en el conjunto de prueba\n",
        "  rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "  print(\"RMSE en conjunto de prueba:\", rmse)\n",
        "  r2 = r2_score(y_test, y_pred)\n",
        "  print(\"Coeficiente de determinación (R2):\", r2)\n",
        "  best_params = grid_search.best_params_\n",
        "  best_model = grid_search.best_estimator_\n",
        "  print(best_params)\n",
        "\n",
        "  dump(best_model, best_model_dir)\n",
        "  dump(best_params, best_param_dir)\n",
        "  results = grid_search.cv_results_\n",
        "  # Guardar los resultados en un archivo\n",
        "  with open(results_dir, 'wb') as f:\n",
        "      pickle.dump(results, f)\n",
        "\n",
        "  mlp_loaded = joblib.load(best_model_dir)\n",
        "  predicciones = mlp_loaded.predict(X_test)\n",
        "  [r2, RMSE, MAE, MAPE] = metrics(y_test, predicciones)\n",
        "  return [r2, RMSE, MAE, MAPE]\n",
        "\n",
        "\n",
        "\n",
        "def RFR_grid(X,y, station_name, fecha, best_model_dir, best_param_dir, time_steps, time_future, results_dir):\n",
        "\n",
        "  X_seq, y_seq = create_sequences2(X, y, time_steps, time_future)\n",
        "  # Dividir los datos en conjunto de entrenamiento y prueba\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)\n",
        "  #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "  # Definir el modelo RandomForestRegressor\n",
        "  rf = RandomForestRegressor(random_state=42)\n",
        "\n",
        "  # Definir el diccionario de parámetros para GridSearchCV\n",
        "  param_grid = {\n",
        "      'n_estimators': [50, 100, 200],\n",
        "      'max_depth': [None, 10, 20, 30],\n",
        "      'min_samples_split': [2, 5, 10],\n",
        "      'min_samples_leaf': [1, 2, 4],\n",
        "      'bootstrap': [True, False]\n",
        "  }\n",
        "\n",
        "  # Configurar GridSearchCV con n_jobs=-1 para usar todos los núcleos disponibles\n",
        "  grid_search = GridSearchCV(rf, param_grid, scoring='r2', cv=5, verbose=1, n_jobs=-1, return_train_score=True)\n",
        "\n",
        "  # Ajustar GridSearchCV a los datos de entrenamiento\n",
        "  grid_search.fit(X_train, y_train)\n",
        "\n",
        "  results = grid_search.cv_results_\n",
        "  # Guardar los resultados en un archivo\n",
        "  with open(results_dir, 'wb') as f:\n",
        "      pickle.dump(results, f)\n",
        "  # Mejor conjunto de parámetros encontrados\n",
        "  print(\"Mejores parámetros:\", grid_search.best_params_)\n",
        "\n",
        "  # Obtener el mejor modelo entrenado\n",
        "  best_model = grid_search.best_estimator_\n",
        "  best_params = grid_search.best_params_\n",
        "  # Predecir en el conjunto de prueba\n",
        "  y_pred = best_model.predict(X_test)\n",
        "  # Calcular el RMSE en el conjunto de prueba\n",
        "  rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "  print(\"RMSE en conjunto de prueba:\", rmse)\n",
        "  r2 = r2_score(y_test, y_pred)\n",
        "  print(\"Coeficiente de determinación (R2):\", r2)\n",
        "\n",
        "  dump(best_model, best_model_dir)\n",
        "  dump(best_params, best_param_dir)\n",
        "\n",
        "  # Cargar el modelo guardado\n",
        "  RFR_loaded = joblib.load(best_model_dir)\n",
        "  # Usar el modelo cargado para hacer predicciones\n",
        "  predicciones = RFR_loaded.predict(X_test)\n",
        "  [r2, RMSE, MAE, MAPE] = metrics(y_test, predicciones)\n",
        "  return [r2, RMSE, MAE, MAPE]\n",
        "\n",
        "\n",
        "def XGBoost_grid(X,y, station_name, fecha, best_xgb_model_dir, best_xgb_params_dir, time_steps, time_future, results_dir):\n",
        "\n",
        "  X_seq, y_seq = create_sequences2(X, y, time_steps, time_future)\n",
        "  # Dividir los datos en conjunto de entrenamiento y prueba\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)\n",
        "  #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "  # Definir el modelo XGBoost\n",
        "  xgb_model = XGBRegressor()\n",
        "\n",
        "  # Definir los parámetros para GridSearchCV\n",
        "  param_grid = {\n",
        "      'n_estimators': [100, 200, 300],\n",
        "      'max_depth': [3, 4, 5],\n",
        "      'learning_rate': [0.01, 0.05, 0.1]\n",
        "  }\n",
        "\n",
        "  # Inicializar GridSearchCV\n",
        "  grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=5, scoring='r2',return_train_score=True)\n",
        "\n",
        "  # Entrenar GridSearchCV\n",
        "  grid_search.fit(X_train, y_train)\n",
        "\n",
        "  results = grid_search.cv_results_\n",
        "  # Guardar los resultados en un archivo\n",
        "  with open(results_dir, 'wb') as f:\n",
        "      pickle.dump(results, f)\n",
        "\n",
        "  # Obtener los mejores parámetros\n",
        "  best_params = grid_search.best_params_\n",
        "  print(\"Mejores parámetros encontrados:\", best_params)\n",
        "\n",
        "  # Entrenar el modelo final con los mejores parámetros\n",
        "  best_xgb_model = XGBRegressor(**best_params)\n",
        "  best_xgb_model.fit(X_train, y_train)\n",
        "\n",
        "  # Evaluar el modelo en el conjunto de prueba\n",
        "  rmse = np.sqrt(mean_squared_error(y_test, best_xgb_model.predict(X_test)))\n",
        "  print(\"RMSE en conjunto de prueba:\", rmse)\n",
        "  # Guardar el modelo\n",
        "  joblib.dump(best_xgb_model, best_xgb_model_dir)\n",
        "  joblib.dump(best_params, best_xgb_params_dir)\n",
        "\n",
        "  loaded_model = joblib.load(best_xgb_model_dir)\n",
        "  predicciones = loaded_model.predict(X_test)\n",
        "  r2 = r2_score(y_test, predicciones)\n",
        "  print(\"Coeficiente de determinación (R2):\", r2)\n",
        "  [r2, RMSE, MAE, MAPE] = metrics(y_test, predicciones)\n",
        "  return [r2, RMSE, MAE, MAPE]\n",
        "\n",
        "\n",
        "def RNN_grid(X,y,station_name,time_steps, fecha, RNN_dir, time_future, results_dir):\n",
        "\n",
        "  # Define tu propio estimador personalizado\n",
        "  class CustomKerasRegressor(BaseEstimator, RegressorMixin):\n",
        "      def __init__(self, units=64, epochs=10, batch_size=32):\n",
        "          self.units = units\n",
        "          self.epochs = epochs\n",
        "          self.batch_size = batch_size\n",
        "          self.model = None\n",
        "\n",
        "      def fit(self, X, y):\n",
        "          input_shape = X.shape[1:]\n",
        "          self.model = Sequential([\n",
        "              SimpleRNN(self.units, input_shape=input_shape),\n",
        "              Dense(1)\n",
        "          ])\n",
        "          #'mean_squared_error'\n",
        "          self.model.compile(optimizer='adam', loss='r2')\n",
        "\n",
        "          # Definir ModelCheckpoint para guardar los mejores pesos\n",
        "          checkpoint_filepath = 'model_checkpoint.h5'\n",
        "          checkpoint_callback = ModelCheckpoint(checkpoint_filepath, save_weights_only=True, save_best_only=True, monitor='val_loss', mode='min', verbose=1, return_train_score=True)\n",
        "\n",
        "          # Entrenar el modelo\n",
        "          self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, validation_split=0.2, callbacks=[checkpoint_callback])\n",
        "\n",
        "          # Cargar los mejores pesos\n",
        "          self.model.load_weights(checkpoint_filepath)\n",
        "          return self\n",
        "\n",
        "      def predict(self, X):\n",
        "          return self.model.predict(X)\n",
        "\n",
        "      def score(self, X, y):\n",
        "          y_pred = self.predict(X)\n",
        "          #return -mean_squared_error(y, y_pred)\n",
        "          return r2_score(y, y_pred)\n",
        "\n",
        "\n",
        "  X_seq, y_seq = create_sequences(X, y, time_steps, time_future)\n",
        "\n",
        "  # Dividir los datos en conjunto de entrenamiento y prueba\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)\n",
        "\n",
        "  # Definir los parámetros a buscar\n",
        "  param_grid = {\n",
        "      'units': [32, 64, 128],\n",
        "      'epochs': [200, 300,400],\n",
        "      'batch_size': [32, 64]\n",
        "  }\n",
        "\n",
        "  # Convertir la métrica de evaluación (MSE)\n",
        "  scoring = make_scorer(mean_squared_error, greater_is_better=False)\n",
        "\n",
        "  # Definir la división de series temporales para cross-validation\n",
        "  tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "  # Crear un modelo Keras personalizado\n",
        "  custom_keras_regressor = CustomKerasRegressor()\n",
        "\n",
        "  # Realizar la búsqueda de hiperparámetros con GridSearchCV\n",
        "  grid_search = GridSearchCV(estimator=custom_keras_regressor, param_grid=param_grid, cv=tscv, scoring=scoring, verbose=1)\n",
        "  grid_search.fit(X_train, y_train)\n",
        "\n",
        "  results = grid_search.cv_results_\n",
        "  # Guardar los resultados en un archivo\n",
        "  with open(results_dir, 'wb') as f:\n",
        "      pickle.dump(results, f)\n",
        "\n",
        "  # Obtener el mejor modelo\n",
        "  best_model = grid_search.best_estimator_\n",
        "  # Guardar el mejor modelo y sus pesos\n",
        "  best_model.model.save(RNN_dir)\n",
        "  # Cargar el mejor modelo y sus pesos\n",
        "  loaded_model = load_model(RNN_dir)\n",
        "  # Evaluación opcional del modelo cargado\n",
        "  #mse = loaded_model.evaluate(X_test, y_test)\n",
        "  #print(\"Mean Squared Error on Test Set:\", mse)\n",
        "\n",
        "  predicciones = loaded_model.predict(X_test)\n",
        "  r2 = r2_score(y_test, y_pred)\n",
        "  print(\"Coeficiente de determinación (R2):\", r2)\n",
        "  [r2, RMSE, MAE, MAPE] = metrics(y_test, predicciones)\n",
        "  return [r2, RMSE, MAE, MAPE]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9R3bh12ZsD5f"
      },
      "source": [
        "## Entrenamos con los datos del tráfico (considerando solo la calle más cercana)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LD_FZbac3VEI",
        "outputId": "ecd16371-2ecc-4d9a-b2f4-34bbf8151581"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['air_traffic_MER.csv', 'air_traffic_UIZ.csv']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "dir = '/content/drive/MyDrive/docto/A24/Tesis A24/Datos/air_pollutionANDtraffic_data'\n",
        "dir_metrics_result = '/content/drive/MyDrive/docto/A24/Tesis A24/modelos/metric_results.csv'\n",
        "files = os.listdir(dir)\n",
        "files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# Obtener la hora actual\n",
        "hora_actual = datetime.now()\n",
        "\n",
        "# Formatear la hora actual\n",
        "hora_formateada = hora_actual.strftime(\"%H:%M:%S\")\n",
        "\n",
        "# Mostrar la hora formateada\n",
        "print(\"La hora actual es:\", hora_formateada)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxXwmecSrMUL",
        "outputId": "56b10d93-ed77-4cc0-8b47-d1615fefa249"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La hora actual es: 01:32:22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqznrLAchIAC"
      },
      "outputs": [],
      "source": [
        "dir = '/content/drive/MyDrive/docto/A24/Tesis A24/Datos/air_pollutionANDtraffic_data'\n",
        "dir_base = '/content/drive/MyDrive/docto/A24/Tesis A24/Modelos/'\n",
        "dir_metrics_result = dir_base + 'models_sequence/metric_results.csv'\n",
        "files = os.listdir(dir)\n",
        "print(files)\n",
        "metric_results_list= []\n",
        "column_names = ['MODEL', 'TRAFFIC_DATA','TARGET', 'STATION','R2', 'RMSE', 'MAE', 'MAPE', 'LEN_X', 'DATE','TIME_STEPS', 'TIME_FUTURE','HORA']\n",
        "for station in files:\n",
        "  target = \"CO\"\n",
        "  X, y, df, file_path = leer_archivo(station, target)\n",
        "  print(file_path)\n",
        "  station_name = station[12:-4]\n",
        "  fecha = '06-06-2024'\n",
        "  time_steps = 12\n",
        "  time_future =1\n",
        "  results_dir = dir_base+'models_sequence/MODELS_air_pollution_AND_traffic_data/results_MLP_'+'timesteps_'+target+'_'+str(time_future)+'_'+'_'+station_name +'_'+fecha+'.pkl'\n",
        "  best_model_dir = dir_base+'models_sequence/MODELS_air_pollution_AND_traffic_data/best_model_SEQUENCE_MLP_'+str(time_steps)+'timesteps_'+target+'_'+station_name +'_'+fecha+'.joblib'\n",
        "  best_param_dir = dir_base+'models_sequence/MODELS_air_pollution_AND_traffic_data/best_params_MLP_'+target+'_'+station_name +'_'+fecha+'.joblib'\n",
        "  [r2, RMSE, MAE, MAPE] = MLP_grid(X,y, station_name, fecha, best_model_dir, best_param_dir, time_steps, time_future, results_dir)\n",
        "  utc_now = datetime.now(pytz.utc)\n",
        "  mexico_city_tz = pytz.timezone('America/Mexico_City')\n",
        "  mexico_city_now = utc_now.astimezone(mexico_city_tz)\n",
        "  hora_formateada = mexico_city_now.strftime('%Y-%m-%d %H:%M')\n",
        "  metric_results_list = []\n",
        "  metric_results_list.append(['MLP', 'NEAREST_STREET', target,station_name, r2, RMSE, MAE, MAPE, len(X), fecha, time_steps, time_future, hora_formateada])\n",
        "  metric_results_df = pd.DataFrame(metric_results_list, columns=column_names)\n",
        "  last_metrics_result = pd.read_csv(dir_metrics_result)\n",
        "  all_metrics = pd.concat([last_metrics_result, metric_results_df])\n",
        "  all_metrics.to_csv(dir_metrics_result, index=False)\n",
        "  results_dir = dir_base+'models_sequence/MODELS_air_pollution_AND_traffic_data/results_RFR_'+'timesteps_'+target+'_'+str(time_future)+'_'+'_'+station_name +'_'+fecha+'.pkl'\n",
        "  best_model_dir = dir_base+'models_sequence/MODELS_air_pollution_AND_traffic_data/best_model_RFR_'+str(time_steps)+'timesteps_'+target+'_'+station_name +'_'+fecha+'.joblib'\n",
        "  best_param_dir = dir_base+'models_sequence/MODELS_air_pollution_AND_traffic_data/best_params_RFR_'+str(time_steps)+'timesteps_'+target+'_'+station_name +'_'+fecha+'.joblib'\n",
        "  metric_results_list = []\n",
        "  [r2, RMSE, MAE, MAPE] = RFR_grid(X,y, station, fecha, best_model_dir, best_param_dir, time_steps, time_future, results_dir)\n",
        "  utc_now = datetime.now(pytz.utc)\n",
        "  mexico_city_tz = pytz.timezone('America/Mexico_City')\n",
        "  mexico_city_now = utc_now.astimezone(mexico_city_tz)\n",
        "  hora_formateada = mexico_city_now.strftime('%Y-%m-%d %H:%M')\n",
        "  metric_results_list.append(['RFR', 'NEAREST_STREET', target,station_name, r2, RMSE, MAE, MAPE, len(X), fecha, time_steps, time_future, hora_formateada])\n",
        "  metric_results_df = pd.DataFrame(metric_results_list, columns=column_names)\n",
        "  last_metrics_result = pd.read_csv(dir_metrics_result)\n",
        "  all_metrics = pd.concat([last_metrics_result, metric_results_df])\n",
        "  all_metrics.to_csv(dir_metrics_result, index=False)\n",
        "  results_dir = dir_base+'models_sequence/MODELS_air_pollution_AND_traffic_data/results_XGB_'+'timesteps_'+target+'_'+str(time_future)+'_'+'_'+station_name +'_'+fecha+'.pkl'\n",
        "  best_xgb_model_dir = dir_base+'models_sequence/MODELS_air_pollution_AND_traffic_data/best_model_XGBoost_'+str(time_steps)+'timesteps_'+target+'_'+station_name +'_'+fecha+'.pkl'\n",
        "  best_xgb_params_dir = dir_base+'models_sequence/MODELS_air_pollution_AND_traffic_data/best_paramas_XGBoost_'+str(time_steps)+'timesteps_'+target+'_'+station_name +'_'+fecha+'.pkl'\n",
        "  [r2, RMSE, MAE, MAPE] = XGBoost_grid(X,y, station, fecha, best_xgb_model_dir, best_xgb_params_dir, time_steps, time_future, results_dir)\n",
        "  utc_now = datetime.now(pytz.utc)\n",
        "  mexico_city_tz = pytz.timezone('America/Mexico_City')\n",
        "  mexico_city_now = utc_now.astimezone(mexico_city_tz)\n",
        "  hora_formateada = mexico_city_now.strftime('%Y-%m-%d %H:%M')\n",
        "  metric_results_list = []\n",
        "  metric_results_list.append(['XGBoost', 'NEAREST_STREET', target,station_name, r2, RMSE, MAE, MAPE, len(X), fecha, time_steps, time_future, hora_formateada])\n",
        "  metric_results_df = pd.DataFrame(metric_results_list, columns=column_names)\n",
        "  last_metrics_result = pd.read_csv(dir_metrics_result)\n",
        "  all_metrics = pd.concat([last_metrics_result, metric_results_df])\n",
        "  all_metrics.to_csv(dir_metrics_result, index=False)\n",
        "  results_dir = dir_base+'models_sequence/MODELS_air_pollution_AND_traffic_data/results_RNN_'+'timesteps_'+target+'_'+str(time_future)+'_'+'_'+station_name +'_'+fecha+'.pkl'\n",
        "  RNN_dir = dir_base+'models_sequence/MODELS_air_pollution_AND_traffic_data/best_model_RNN_'+str(time_steps)+'timesteps_'+target+'_'+station_name+'_'+fecha+'.h5'\n",
        "  #[r2, RMSE, MAE, MAPE] = RNN_grid(X,y, station, time_steps, fecha, RNN_dir, time_future, results_dir)\n",
        "  #utc_now = datetime.now(pytz.utc)\n",
        "  #mexico_city_tz = pytz.timezone('America/Mexico_City')\n",
        "  #mexico_city_now = utc_now.astimezone(mexico_city_tz)\n",
        "  #hora_formateada = mexico_city_now.strftime('%Y-%m-%d %H:%M')\n",
        "  #metric_results_list = []\n",
        "  #metric_results_list.append(['RNN', 'NEAREST_STREET', target,station_name, r2, RMSE, MAE, MAPE, len(X), fecha, time_future, hora_formateada])\n",
        "  #metric_results_df = pd.DataFrame(metric_results_list, columns=column_names)\n",
        "  #last_metrics_result = pd.read_csv(dir_metrics_result)\n",
        "  #all_metrics = pd.concat([last_metrics_result, metric_results_df])\n",
        "  #all_metrics.to_csv(dir_metrics_result, index=False)\n",
        "  print(all_metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXIo3El7r-qQ"
      },
      "source": [
        "## Ahora entrenamos los mismos modelos pero sin los datos del tráfico"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hora_actual = datetime.now()\n",
        "hora_formateada = hora_actual.strftime(\"%H:%M:%S\")\n",
        "hora_formateada"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mBIxyi5u51RL",
        "outputId": "197d9813-4f0c-421f-fbc8-084578579024"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'04:14:11'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGYWnxKFrrH1",
        "outputId": "f34fc32a-fd88-4f9e-c4af-9dcc902d07c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['air_traffic_MER.csv', 'air_traffic_UIZ.csv']\n",
            "/content/drive/MyDrive/docto/A24/Tesis A24/Datos/air_pollutionANDtraffic_data/air_traffic_MER.csv\n",
            "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mejores parámetros: {'bootstrap': True, 'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "RMSE en conjunto de prueba: 0.08909356866754128\n",
            "Coeficiente de determinación (R2): 0.4975580332315055\n",
            "R^2: 0.4975580332315055\n",
            "RMSE 0.08909356866754128\n",
            "MAE: 0.066229401448209\n",
            "MAPE: 617540268.1830554 %\n",
            "      MODEL    TRAFFIC_DATA TARGET STATION        DATE        R2      RMSE  \\\n",
            "0       NaN             NaN    NaN     NaN         NaN       NaN       NaN   \n",
            "1       NaN             NaN    NaN     NaN         NaN       NaN       NaN   \n",
            "2       NaN             NaN    NaN     NaN         NaN       NaN       NaN   \n",
            "3       NaN             NaN    NaN     NaN         NaN       NaN       NaN   \n",
            "4       NaN             NaN    NaN     NaN         NaN       NaN       NaN   \n",
            "5       NaN             NaN    NaN     NaN         NaN       NaN       NaN   \n",
            "6       NaN             NaN    NaN     NaN         NaN       NaN       NaN   \n",
            "7       NaN             NaN    NaN     NaN         NaN       NaN       NaN   \n",
            "8       NaN             NaN    NaN     NaN         NaN       NaN       NaN   \n",
            "9       NaN             NaN    NaN     NaN         NaN       NaN       NaN   \n",
            "10      NaN             NaN    NaN     NaN         NaN       NaN       NaN   \n",
            "11      NaN             NaN    NaN     NaN         NaN       NaN       NaN   \n",
            "12  XGBoost  NEAREST_STREET    NO2     UIZ  06-06-2024  0.536870  0.103849   \n",
            "0       RFR  NEAREST_STREET    NO2     MER  06-06-2024  0.497558  0.089094   \n",
            "\n",
            "         MAE          MAPE   LEN_X  TIME_STEPS  TIME_FUTURE              HORA  \n",
            "0        NaN           NaN     NaN         NaN          NaN               NaN  \n",
            "1        NaN           NaN     NaN         NaN          NaN               NaN  \n",
            "2        NaN           NaN     NaN         NaN          NaN               NaN  \n",
            "3        NaN           NaN     NaN         NaN          NaN               NaN  \n",
            "4        NaN           NaN     NaN         NaN          NaN               NaN  \n",
            "5        NaN           NaN     NaN         NaN          NaN               NaN  \n",
            "6        NaN           NaN     NaN         NaN          NaN               NaN  \n",
            "7        NaN           NaN     NaN         NaN          NaN               NaN  \n",
            "8        NaN           NaN     NaN         NaN          NaN               NaN  \n",
            "9        NaN           NaN     NaN         NaN          NaN               NaN  \n",
            "10       NaN           NaN     NaN         NaN          NaN               NaN  \n",
            "11       NaN           NaN     NaN         NaN          NaN               NaN  \n",
            "12  0.076942  7.163964e+08  1090.0        12.0          1.0          04:06:48  \n",
            "0   0.066229  6.175403e+08   950.0        12.0          1.0  2024-06-06 23:24  \n"
          ]
        }
      ],
      "source": [
        "dir = '/content/drive/MyDrive/docto/A24/Tesis A24/Datos/air_pollutionANDtraffic_data'\n",
        "dir_base = '/content/drive/MyDrive/docto/A24/Tesis A24/Modelos/'\n",
        "dir_metrics_result = dir_base + 'models_sequence/metric_results.csv'\n",
        "files = os.listdir(dir)\n",
        "print(files)\n",
        "metric_results_list= []\n",
        "column_names = ['MODEL', 'TRAFFIC_DATA','TARGET', 'STATION','R2', 'RMSE', 'MAE', 'MAPE', 'LEN_X', 'DATE','TIME_STEPS', 'TIME_FUTURE','HORA']\n",
        "for station in [files[0]]:\n",
        "  target = \"NO2\"\n",
        "  X, y, df, file_path = leer_archivo(station, target)\n",
        "  X = X.drop(columns=['traffic'])\n",
        "  print(file_path)\n",
        "  station_name = station[12:-4]\n",
        "  fecha = '06-06-2024'\n",
        "  time_steps = 12\n",
        "  time_future = 1\n",
        "  \"\"\"\n",
        "  results_dir = dir_base+'models_sequence/MODELS_air_pollution/results_MLP_'+'timesteps_'+target+'_'+str(time_future)+'_'+'_'+station_name +'_'+fecha+'.pkl'\n",
        "  best_model_dir = dir_base+'models_sequence/MODELS_air_pollution/best_model_SEQUENCE_MLP_'+str(time_steps)+'timesteps_'+target+'_'+station_name +'_'+fecha+'.joblib'\n",
        "  best_param_dir = dir_base+'models_sequence/MODELS_air_pollution/best_params_MLP_'+target+'_'+station_name +'_'+fecha+'.joblib'\n",
        "  [r2, RMSE, MAE, MAPE] = MLP_grid(X,y, station_name, fecha, best_model_dir, best_param_dir, time_steps, time_future, results_dir)\n",
        "  utc_now = datetime.now(pytz.utc)\n",
        "  mexico_city_tz = pytz.timezone('America/Mexico_City')\n",
        "  mexico_city_now = utc_now.astimezone(mexico_city_tz)\n",
        "  hora_formateada = mexico_city_now.strftime('%Y-%m-%d %H:%M')\n",
        "  metric_results_list = []\n",
        "  metric_results_list.append(['MLP', 'NEAREST_STREET', target,station_name, r2, RMSE, MAE, MAPE, len(X), fecha, time_steps, time_future, hora_formateada])\n",
        "  metric_results_df = pd.DataFrame(metric_results_list, columns=column_names)\n",
        "  last_metrics_result = pd.read_csv(dir_metrics_result)\n",
        "  all_metrics = pd.concat([last_metrics_result, metric_results_df])\n",
        "  all_metrics.to_csv(dir_metrics_result, index=False)\n",
        "  \"\"\"\n",
        "  results_dir = dir_base+'models_sequence/MODELS_air_pollution/results_RFR_'+'timesteps_'+target+'_'+str(time_future)+'_'+'_'+station_name +'_'+fecha+'.pkl'\n",
        "  best_model_dir = dir_base+'models_sequence/MODELS_air_pollution/best_model_RFR_'+str(time_steps)+'timesteps_'+target+'_'+station_name +'_'+fecha+'.joblib'\n",
        "  best_param_dir = dir_base+'models_sequence/MODELS_air_pollution/best_params_RFR_'+str(time_steps)+'timesteps_'+target+'_'+station_name +'_'+fecha+'.joblib'\n",
        "\n",
        "  metric_results_list = []\n",
        "  [r2, RMSE, MAE, MAPE] = RFR_grid(X,y, station, fecha, best_model_dir, best_param_dir, time_steps, time_future, results_dir)\n",
        "  utc_now = datetime.now(pytz.utc)\n",
        "  mexico_city_tz = pytz.timezone('America/Mexico_City')\n",
        "  mexico_city_now = utc_now.astimezone(mexico_city_tz)\n",
        "  hora_formateada = mexico_city_now.strftime('%Y-%m-%d %H:%M')\n",
        "  metric_results_list.append(['RFR', 'NEAREST_STREET', target,station_name, r2, RMSE, MAE, MAPE, len(X), fecha, time_steps, time_future, hora_formateada])\n",
        "  metric_results_df = pd.DataFrame(metric_results_list, columns=column_names)\n",
        "  last_metrics_result = pd.read_csv(dir_metrics_result)\n",
        "  all_metrics = pd.concat([last_metrics_result, metric_results_df])\n",
        "  all_metrics.to_csv(dir_metrics_result, index=False)\n",
        "  \"\"\"\n",
        "  results_dir = dir_base+'models_sequence/MODELS_air_pollution/results_XGB_'+'timesteps_'+target+'_'+str(time_future)+'_'+'_'+station_name +'_'+fecha+'.pkl'\n",
        "  best_xgb_model_dir = dir_base+'models_sequence/MODELS_air_pollution/best_model_XGBoost_'+str(time_steps)+'timesteps_'+target+'_'+station_name +'_'+fecha+'.pkl'\n",
        "  best_xgb_params_dir = dir_base+'models_sequence/MODELS_air_pollution/best_paramas_XGBoost_'+str(time_steps)+'timesteps_'+target+'_'+station_name +'_'+fecha+'.pkl'\n",
        "  [r2, RMSE, MAE, MAPE] = XGBoost_grid(X,y, station, fecha, best_xgb_model_dir, best_xgb_params_dir, time_steps, time_future, results_dir)\n",
        "  utc_now = datetime.now(pytz.utc)\n",
        "  mexico_city_tz = pytz.timezone('America/Mexico_City')\n",
        "  mexico_city_now = utc_now.astimezone(mexico_city_tz)\n",
        "  hora_formateada = mexico_city_now.strftime('%Y-%m-%d %H:%M')\n",
        "  metric_results_list = []\n",
        "  metric_results_list.append(['XGBoost', 'NEAREST_STREET', target,station_name, r2, RMSE, MAE, MAPE, len(X), fecha, time_steps, time_future, hora_formateada])\n",
        "  metric_results_df = pd.DataFrame(metric_results_list, columns=column_names)\n",
        "  last_metrics_result = pd.read_csv(dir_metrics_result)\n",
        "  all_metrics = pd.concat([last_metrics_result, metric_results_df])\n",
        "  all_metrics.to_csv(dir_metrics_result, index=False)\n",
        "  results_dir = dir_base+'models_sequence/MODELS_air_pollution/results_RNN_'+'timesteps_'+target+'_'+str(time_future)+'_'+'_'+station_name +'_'+fecha+'.pkl'\n",
        "  RNN_dir = dir_base+'models_sequence/MODELS_air_pollution/best_model_RNN_'+str(time_steps)+'timesteps_'+target+'_'+station_name+'_'+fecha+'.h5'\n",
        "  #[r2, RMSE, MAE, MAPE] = RNN_grid(X,y, station, time_steps, fecha, RNN_dir, time_future, results_dir)\n",
        "  #utc_now = datetime.now(pytz.utc)\n",
        "  #mexico_city_tz = pytz.timezone('America/Mexico_City')\n",
        "  #mexico_city_now = utc_now.astimezone(mexico_city_tz)\n",
        "  #hora_formateada = mexico_city_now.strftime('%Y-%m-%d %H:%M')\n",
        "  #metric_results_list = []\n",
        "  #metric_results_list.append(['RNN', 'NEAREST_STREET', target,station_name, r2, RMSE, MAE, MAPE, len(X), fecha, time_future, hora_formateada])\n",
        "  #metric_results_df = pd.DataFrame(metric_results_list, columns=column_names)\n",
        "  #last_metrics_result = pd.read_csv(dir_metrics_result)\n",
        "  #all_metrics = pd.concat([last_metrics_result, metric_results_df])\n",
        "  #all_metrics.to_csv(dir_metrics_result, index=False)\n",
        "  \"\"\"\n",
        "  print(all_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "dir = '/content/drive/MyDrive/docto/A24/Tesis A24/Modelos/models_sequence/MODELS_air_pollution_AND_traffic_data/results_XGB_CO_UIZ_06-06-2024.pkl'\n",
        "# Cargar los resultados desde el archivo\n",
        "with open(dir, 'rb') as f:\n",
        "    loaded_results = pickle.load(f)\n",
        "\n",
        "# Mostrar los resultados de los puntajes de entrenamiento\n",
        "train_scores = loaded_results['mean_train_score']\n",
        "print(\"Mean train scores for each parameter combination:\")\n",
        "for mean_train_score in train_scores:\n",
        "    print(mean_train_score)\n"
      ],
      "metadata": {
        "id": "UnD-xYK8TFAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [3, 4, 5],\n",
        "    'learning_rate': [0.01, 0.05, 0.1]\n",
        "}\n",
        "\n",
        "# Cargar un conjunto de datos de ejemplo\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Definir un modelo\n",
        "model = RandomForestClassifier()\n",
        "\n",
        "# Definir una rejilla de parámetros\n",
        "param_grid = {\n",
        "    'n_estimators': [10, 50, 100],\n",
        "    'max_depth': [1, 10, 20]\n",
        "}\n",
        "\n",
        "# Crear el objeto GridSearchCV con return_train_score=True\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, return_train_score=True)\n",
        "\n",
        "# Ajustar el modelo a los datos\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Obtener los resultados\n",
        "results = grid_search.cv_results_\n",
        "\n",
        "#results = loaded_results\n",
        "# Extraer los puntajes de entrenamiento y validación\n",
        "train_scores = np.array(results['mean_train_score']).reshape(len(param_grid['max_depth']), len(param_grid['n_estimators']))\n",
        "test_scores = np.array(results['mean_test_score']).reshape(len(param_grid['max_depth']), len(param_grid['n_estimators']))\n",
        "\n",
        "# Graficar los resultados\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# Crear rejilla de parámetros\n",
        "n_estimators, max_depth = np.meshgrid(param_grid['n_estimators'], param_grid['max_depth'])\n",
        "\n",
        "# Graficar superficies\n",
        "ax.plot_surface(n_estimators, max_depth, train_scores, cmap='viridis', label='Train Score')\n",
        "ax.plot_surface(n_estimators, max_depth, test_scores, cmap='plasma', label='Test Score')\n",
        "\n",
        "# Etiquetas y título\n",
        "ax.set_xlabel('Number of Estimators')\n",
        "ax.set_ylabel('Max Depth')\n",
        "ax.set_zlabel('Mean Score')\n",
        "ax.set_title('Grid Search Results')\n",
        "\n",
        "# Añadir leyenda\n",
        "ax.legend()\n",
        "\n",
        "# Mostrar la gráfica\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dyzL_tWnTZWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "id": "DWqurJ4UTjEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6G0oZAW3UNBH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}