{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gKXm0WfsNDg",
        "outputId": "a5fed3b7-5050-49b7-b1bf-b1d5fce962d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GW6CJkfV_pqn",
        "outputId": "b593db3a-b577-4659-deb3-33c223a406c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.2.2\n"
          ]
        }
      ],
      "source": [
        "import sklearn\n",
        "print(sklearn.__version__)\n",
        "# Debe ser version 1.2.2 de lo contrario ejecutar:\n",
        "#!pip install scikit-learn==1.2.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kChVpEWGwc5Z"
      },
      "outputs": [],
      "source": [
        "def metrics(y_test, predicciones):\n",
        "\n",
        "    # Coeficiente de determinación (R-cuadrado)\n",
        "    r2 = r2_score(y_test, predicciones)\n",
        "    # Error cuadrado medio\n",
        "    rmse = mean_squared_error(y_true  = y_test, y_pred  = predicciones, squared = False)\n",
        "    # Error Absoluto Medio\n",
        "    mae = mean_absolute_error(y_test, predicciones)\n",
        "    # Error Porcentual Absoluto Medio\n",
        "    # mape = np.mean(np.abs((y_test - predicciones) / y_test)) * 100\n",
        "    epsilon=1e-10\n",
        "    mape = np.mean(np.abs((y_test - predicciones) / (y_test + epsilon))) * 100\n",
        "\n",
        "    print(\"R^2:\", r2)\n",
        "    print(\"RMSE\", rmse)\n",
        "    print(\"MAE:\", mae)\n",
        "    print(\"MAPE:\", mape, \"%\")\n",
        "\n",
        "    r2 = round(r2, 6)\n",
        "    rmse = round(rmse, 6)\n",
        "    mae = round(mae, 6)\n",
        "    mape = round(mape, 6)\n",
        "    return r2, rmse, mae, mape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e5kzJ1JeUTA"
      },
      "source": [
        "# Todos los modelos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "99SMQHaReWJo"
      },
      "outputs": [],
      "source": [
        "import openpyxl\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from math import sqrt\n",
        "import matplotlib.dates as mdates\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import r2_score\n",
        "from joblib import dump\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import make_scorer, mean_squared_error, r2_score\n",
        "\n",
        "# RNN\n",
        "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense\n",
        "from sklearn.metrics import make_scorer, mean_squared_error\n",
        "from sklearn.base import BaseEstimator, RegressorMixin\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "import pickle\n",
        "import pytz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener la hora actual en UTC\n",
        "utc_now = datetime.now(pytz.utc)\n",
        "mexico_city_tz = pytz.timezone('America/Mexico_City')\n",
        "mexico_city_now = utc_now.astimezone(mexico_city_tz)\n",
        "hora_formateada = mexico_city_now.strftime('%Y-%m-%d %H:%M')\n",
        "print(\"Hora actual en Ciudad de México:\", hora_formateada)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiJS5xx46Pa6",
        "outputId": "15e3b5ca-2089-4d0f-f504-6d82f015e386"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hora actual en Ciudad de México: 2024-06-07 12:58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "AXQazGjzhmBS"
      },
      "outputs": [],
      "source": [
        "def leer_archivo(station, target):\n",
        "\n",
        "  #file = files[3]\n",
        "  file_path = os.path.join(dir,station)\n",
        "  df  = pd.read_csv(file_path)\n",
        "  dates = df.date\n",
        "  y = df[target]\n",
        "  X = df.drop(columns=['date', 'year', 'month', 'day', 'hour', 'minute'])\n",
        "  X = X.drop(columns=[target])\n",
        "\n",
        "  return X, y, df, file_path\n",
        "\n",
        "\n",
        "# Convertir los datos en secuencias adecuadas para RNN\n",
        "def create_sequences(X, y, time_steps, time_future):\n",
        "    Xs, ys = [], []\n",
        "    for i in range(len(X) - time_steps):\n",
        "        Xs.append(X[i:(i + time_steps)])\n",
        "        ys.append(y[i + time_steps+ time_future])\n",
        "    return np.array(Xs), np.array(ys)\n",
        "\n",
        "\n",
        "def create_sequences2(X, y, time_steps, time_future):\n",
        "  Xs, ys = [], []\n",
        "  for i in range(len(X) - time_steps-time_future):\n",
        "    df = X[i:(i + time_steps)]\n",
        "    array = df.to_numpy()\n",
        "    # Aplanar el array a un vector\n",
        "    vector = array.flatten()\n",
        "    Xs.append(vector)\n",
        "    ys.append(y[i + time_steps+time_future])\n",
        "  return np.array(Xs), np.array(ys)\n",
        "\n",
        "def MLP_grid(X,y, station_name, fecha, best_model_dir, best_param_dir, time_steps, time_future, results_dir):\n",
        "\n",
        "  X_seq, y_seq = create_sequences2(X, y, time_steps, time_future)\n",
        "  # Dividir los datos en conjunto de entrenamiento y prueba\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)\n",
        "  # Dividir los datos en conjunto de entrenamiento y prueba\n",
        "  #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "  # Definir el regresor MLP\n",
        "  #mlp = MLPRegressor(max_iter=1000)\n",
        "  mlp = MLPRegressor(max_iter=1000)\n",
        "\n",
        "  # Definir el diccionario de parámetros para GridSearchCV\n",
        "  param_grid = {\n",
        "      'hidden_layer_sizes': [(10,), (50,), (100,), (50, 50), (100,100), (50,50,50), (100,100,100), (75,75,75,75), (25,25,25,25), (100,150,100,25)],\n",
        "      'activation': ['relu', 'tanh'],\n",
        "      'solver': ['adam', 'sgd'],\n",
        "      'alpha': [0.0001, 0.05],\n",
        "      'learning_rate': ['constant', 'adaptive'],\n",
        "      'max_iter': [200, 500, 1000, 1500],  # Especificar diferentes números de iteraciones\n",
        "  }\n",
        "\n",
        "  # Configurar GridSearchCV\n",
        "\n",
        "  #'neg_mean_squared_error'\n",
        "  grid_search = GridSearchCV(mlp, param_grid, scoring='r2', cv=5, n_jobs=-1, verbose=1 ,return_train_score=True)\n",
        "  # Ajustar GridSearchCV a los datos de entrenamiento\n",
        "  grid_search.fit(X_train, y_train)\n",
        "\n",
        "  # Mejores parámetros encontrados\n",
        "  print(\"Mejores parámetros:\", grid_search.best_params_)\n",
        "\n",
        "  # Obtener el mejor modelo entrenado\n",
        "  best_model = grid_search.best_estimator_\n",
        "\n",
        "  # Predecir en el conjunto de prueba\n",
        "  y_pred = best_model.predict(X_test)\n",
        "\n",
        "  # Calcular el RMSE en el conjunto de prueba\n",
        "  rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "  print(\"RMSE en conjunto de prueba:\", rmse)\n",
        "  r2 = r2_score(y_test, y_pred)\n",
        "  print(\"Coeficiente de determinación (R2):\", r2)\n",
        "  best_params = grid_search.best_params_\n",
        "  best_model = grid_search.best_estimator_\n",
        "  print(best_params)\n",
        "\n",
        "  dump(best_model, best_model_dir)\n",
        "  dump(best_params, best_param_dir)\n",
        "  results = grid_search.cv_results_\n",
        "  # Guardar los resultados en un archivo\n",
        "  with open(results_dir, 'wb') as f:\n",
        "      pickle.dump(results, f)\n",
        "\n",
        "  mlp_loaded = joblib.load(best_model_dir)\n",
        "  predicciones = mlp_loaded.predict(X_test)\n",
        "  [r2, RMSE, MAE, MAPE] = metrics(y_test, predicciones)\n",
        "  return [r2, RMSE, MAE, MAPE]\n",
        "\n",
        "\n",
        "\n",
        "def RFR_grid(X,y, station_name, fecha, best_model_dir, best_param_dir, time_steps, time_future, results_dir):\n",
        "\n",
        "  X_seq, y_seq = create_sequences2(X, y, time_steps, time_future)\n",
        "  # Dividir los datos en conjunto de entrenamiento y prueba\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)\n",
        "  #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "  # Definir el modelo RandomForestRegressor\n",
        "  rf = RandomForestRegressor(random_state=42)\n",
        "\n",
        "  # Definir el diccionario de parámetros para GridSearchCV\n",
        "  param_grid = {\n",
        "      'n_estimators': [50, 100, 200],\n",
        "      'max_depth': [None, 10, 20, 30],\n",
        "      'min_samples_split': [2, 5, 10],\n",
        "      'min_samples_leaf': [1, 2, 4],\n",
        "      'bootstrap': [True, False]\n",
        "  }\n",
        "\n",
        "  # Configurar GridSearchCV con n_jobs=-1 para usar todos los núcleos disponibles\n",
        "  grid_search = GridSearchCV(rf, param_grid, scoring='r2', cv=5, verbose=1, n_jobs=-1, return_train_score=True)\n",
        "\n",
        "  # Ajustar GridSearchCV a los datos de entrenamiento\n",
        "  grid_search.fit(X_train, y_train)\n",
        "\n",
        "  results = grid_search.cv_results_\n",
        "  # Guardar los resultados en un archivo\n",
        "  with open(results_dir, 'wb') as f:\n",
        "      pickle.dump(results, f)\n",
        "  # Mejor conjunto de parámetros encontrados\n",
        "  print(\"Mejores parámetros:\", grid_search.best_params_)\n",
        "\n",
        "  # Obtener el mejor modelo entrenado\n",
        "  best_model = grid_search.best_estimator_\n",
        "  best_params = grid_search.best_params_\n",
        "  # Predecir en el conjunto de prueba\n",
        "  y_pred = best_model.predict(X_test)\n",
        "  # Calcular el RMSE en el conjunto de prueba\n",
        "  rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "  print(\"RMSE en conjunto de prueba:\", rmse)\n",
        "  r2 = r2_score(y_test, y_pred)\n",
        "  print(\"Coeficiente de determinación (R2):\", r2)\n",
        "\n",
        "  dump(best_model, best_model_dir)\n",
        "  dump(best_params, best_param_dir)\n",
        "\n",
        "  # Cargar el modelo guardado\n",
        "  RFR_loaded = joblib.load(best_model_dir)\n",
        "  # Usar el modelo cargado para hacer predicciones\n",
        "  predicciones = RFR_loaded.predict(X_test)\n",
        "  [r2, RMSE, MAE, MAPE] = metrics(y_test, predicciones)\n",
        "  return [r2, RMSE, MAE, MAPE]\n",
        "\n",
        "\n",
        "def XGBoost_grid(X,y, station_name, fecha, best_xgb_model_dir, best_xgb_params_dir, time_steps, time_future, results_dir):\n",
        "\n",
        "  X_seq, y_seq = create_sequences2(X, y, time_steps, time_future)\n",
        "  # Dividir los datos en conjunto de entrenamiento y prueba\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)\n",
        "  #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "  # Definir el modelo XGBoost\n",
        "  xgb_model = XGBRegressor()\n",
        "\n",
        "  # Definir los parámetros para GridSearchCV\n",
        "  param_grid = {\n",
        "      'n_estimators': [100, 200, 300],\n",
        "      'max_depth': [3, 4, 5],\n",
        "      'learning_rate': [0.01, 0.05, 0.1]\n",
        "  }\n",
        "\n",
        "  # Inicializar GridSearchCV\n",
        "  grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=5, scoring='r2',return_train_score=True)\n",
        "\n",
        "  # Entrenar GridSearchCV\n",
        "  grid_search.fit(X_train, y_train)\n",
        "\n",
        "  results = grid_search.cv_results_\n",
        "  # Guardar los resultados en un archivo\n",
        "  with open(results_dir, 'wb') as f:\n",
        "      pickle.dump(results, f)\n",
        "\n",
        "  # Obtener los mejores parámetros\n",
        "  best_params = grid_search.best_params_\n",
        "  print(\"Mejores parámetros encontrados:\", best_params)\n",
        "\n",
        "  # Entrenar el modelo final con los mejores parámetros\n",
        "  best_xgb_model = XGBRegressor(**best_params)\n",
        "  best_xgb_model.fit(X_train, y_train)\n",
        "\n",
        "  # Evaluar el modelo en el conjunto de prueba\n",
        "  rmse = np.sqrt(mean_squared_error(y_test, best_xgb_model.predict(X_test)))\n",
        "  print(\"RMSE en conjunto de prueba:\", rmse)\n",
        "  # Guardar el modelo\n",
        "  joblib.dump(best_xgb_model, best_xgb_model_dir)\n",
        "  joblib.dump(best_params, best_xgb_params_dir)\n",
        "\n",
        "  loaded_model = joblib.load(best_xgb_model_dir)\n",
        "  predicciones = loaded_model.predict(X_test)\n",
        "  r2 = r2_score(y_test, predicciones)\n",
        "  print(\"Coeficiente de determinación (R2):\", r2)\n",
        "  [r2, RMSE, MAE, MAPE] = metrics(y_test, predicciones)\n",
        "  return [r2, RMSE, MAE, MAPE]\n",
        "\n",
        "\n",
        "def RNN_grid(X,y,station_name,time_steps, fecha, RNN_dir, time_future, results_dir):\n",
        "\n",
        "  # Define tu propio estimador personalizado\n",
        "  class CustomKerasRegressor(BaseEstimator, RegressorMixin):\n",
        "      def __init__(self, units=64, epochs=10, batch_size=32):\n",
        "          self.units = units\n",
        "          self.epochs = epochs\n",
        "          self.batch_size = batch_size\n",
        "          self.model = None\n",
        "\n",
        "      def fit(self, X, y):\n",
        "          input_shape = X.shape[1:]\n",
        "          self.model = Sequential([\n",
        "              SimpleRNN(self.units, input_shape=input_shape),\n",
        "              Dense(1)\n",
        "          ])\n",
        "          #'mean_squared_error'\n",
        "          self.model.compile(optimizer='adam', loss='r2')\n",
        "\n",
        "          # Definir ModelCheckpoint para guardar los mejores pesos\n",
        "          checkpoint_filepath = 'model_checkpoint.h5'\n",
        "          checkpoint_callback = ModelCheckpoint(checkpoint_filepath, save_weights_only=True, save_best_only=True, monitor='val_loss', mode='min', verbose=1, return_train_score=True)\n",
        "\n",
        "          # Entrenar el modelo\n",
        "          self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, validation_split=0.2, callbacks=[checkpoint_callback])\n",
        "\n",
        "          # Cargar los mejores pesos\n",
        "          self.model.load_weights(checkpoint_filepath)\n",
        "          return self\n",
        "\n",
        "      def predict(self, X):\n",
        "          return self.model.predict(X)\n",
        "\n",
        "      def score(self, X, y):\n",
        "          y_pred = self.predict(X)\n",
        "          #return -mean_squared_error(y, y_pred)\n",
        "          return r2_score(y, y_pred)\n",
        "\n",
        "\n",
        "  X_seq, y_seq = create_sequences(X, y, time_steps, time_future)\n",
        "\n",
        "  # Dividir los datos en conjunto de entrenamiento y prueba\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)\n",
        "\n",
        "  # Definir los parámetros a buscar\n",
        "  param_grid = {\n",
        "      'units': [32, 64, 128],\n",
        "      'epochs': [200, 300,400],\n",
        "      'batch_size': [32, 64]\n",
        "  }\n",
        "\n",
        "  # Convertir la métrica de evaluación (MSE)\n",
        "  scoring = make_scorer(mean_squared_error, greater_is_better=False)\n",
        "\n",
        "  # Definir la división de series temporales para cross-validation\n",
        "  tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "  # Crear un modelo Keras personalizado\n",
        "  custom_keras_regressor = CustomKerasRegressor()\n",
        "\n",
        "  # Realizar la búsqueda de hiperparámetros con GridSearchCV\n",
        "  grid_search = GridSearchCV(estimator=custom_keras_regressor, param_grid=param_grid, cv=tscv, scoring=scoring, verbose=1)\n",
        "  grid_search.fit(X_train, y_train)\n",
        "\n",
        "  results = grid_search.cv_results_\n",
        "  # Guardar los resultados en un archivo\n",
        "  with open(results_dir, 'wb') as f:\n",
        "      pickle.dump(results, f)\n",
        "\n",
        "  # Obtener el mejor modelo\n",
        "  best_model = grid_search.best_estimator_\n",
        "  # Guardar el mejor modelo y sus pesos\n",
        "  best_model.model.save(RNN_dir)\n",
        "  # Cargar el mejor modelo y sus pesos\n",
        "  loaded_model = load_model(RNN_dir)\n",
        "  # Evaluación opcional del modelo cargado\n",
        "  #mse = loaded_model.evaluate(X_test, y_test)\n",
        "  #print(\"Mean Squared Error on Test Set:\", mse)\n",
        "\n",
        "  predicciones = loaded_model.predict(X_test)\n",
        "  r2 = r2_score(y_test, predicciones)\n",
        "  print(\"Coeficiente de determinación (R2):\", r2)\n",
        "  [r2, RMSE, MAE, MAPE] = metrics(y_test, predicciones)\n",
        "  return [r2, RMSE, MAE, MAPE]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9R3bh12ZsD5f"
      },
      "source": [
        "## Entrenamos con los datos del tráfico (considerando solo la calle más cercana)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LD_FZbac3VEI",
        "outputId": "ecd16371-2ecc-4d9a-b2f4-34bbf8151581"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['air_traffic_MER.csv', 'air_traffic_UIZ.csv']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "dir = '/content/drive/MyDrive/docto/A24/Tesis A24/Datos/air_pollutionANDtraffic_data'\n",
        "dir_metrics_result = '/content/drive/MyDrive/docto/A24/Tesis A24/modelos/metric_results.csv'\n",
        "files = os.listdir(dir)\n",
        "files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqznrLAchIAC"
      },
      "outputs": [],
      "source": [
        "dir = '/content/drive/MyDrive/docto/A24/Tesis A24/Datos/air_pollutionANDtraffic_data'\n",
        "dir_base = '/content/drive/MyDrive/docto/A24/Tesis A24/Modelos/'\n",
        "dir_metrics_result = dir_base + 'models_sequence/metric_results.csv'\n",
        "files = os.listdir(dir)\n",
        "print(files)\n",
        "metric_results_list= []\n",
        "column_names = ['MODEL', 'TRAFFIC_DATA','TARGET', 'STATION','R2', 'RMSE', 'MAE', 'MAPE', 'LEN_X', 'DATE','TIME_STEPS', 'TIME_FUTURE','HORA']\n",
        "for station in files:\n",
        "  target = \"CO\"\n",
        "  X, y, df, file_path = leer_archivo(station, target)\n",
        "  print(file_path)\n",
        "  station_name = station[12:-4]\n",
        "  fecha = '06-06-2024'\n",
        "  time_steps = 12\n",
        "  time_future =1\n",
        "  results_dir = dir_base+'models_sequence/MODELS_air_pollution_AND_traffic_data/results_MLP_'+'timesteps_'+target+'_'+str(time_future)+'_'+'_'+station_name +'_'+fecha+'.pkl'\n",
        "  best_model_dir = dir_base+'models_sequence/MODELS_air_pollution_AND_traffic_data/best_model_SEQUENCE_MLP_'+str(time_steps)+'timesteps_'+target+'_'+station_name +'_'+fecha+'.joblib'\n",
        "  best_param_dir = dir_base+'models_sequence/MODELS_air_pollution_AND_traffic_data/best_params_MLP_'+target+'_'+station_name +'_'+fecha+'.joblib'\n",
        "  [r2, RMSE, MAE, MAPE] = MLP_grid(X,y, station_name, fecha, best_model_dir, best_param_dir, time_steps, time_future, results_dir)\n",
        "  utc_now = datetime.now(pytz.utc)\n",
        "  mexico_city_tz = pytz.timezone('America/Mexico_City')\n",
        "  mexico_city_now = utc_now.astimezone(mexico_city_tz)\n",
        "  hora_formateada = mexico_city_now.strftime('%Y-%m-%d %H:%M')\n",
        "  metric_results_list = []\n",
        "  metric_results_list.append(['MLP', 'NEAREST_STREET', target,station_name, r2, RMSE, MAE, MAPE, len(X), fecha, time_steps, time_future, hora_formateada])\n",
        "  metric_results_df = pd.DataFrame(metric_results_list, columns=column_names)\n",
        "  last_metrics_result = pd.read_csv(dir_metrics_result)\n",
        "  all_metrics = pd.concat([last_metrics_result, metric_results_df])\n",
        "  all_metrics.to_csv(dir_metrics_result, index=False)\n",
        "  results_dir = dir_base+'models_sequence/MODELS_air_pollution_AND_traffic_data/results_RFR_'+'timesteps_'+target+'_'+str(time_future)+'_'+'_'+station_name +'_'+fecha+'.pkl'\n",
        "  best_model_dir = dir_base+'models_sequence/MODELS_air_pollution_AND_traffic_data/best_model_RFR_'+str(time_steps)+'timesteps_'+target+'_'+station_name +'_'+fecha+'.joblib'\n",
        "  best_param_dir = dir_base+'models_sequence/MODELS_air_pollution_AND_traffic_data/best_params_RFR_'+str(time_steps)+'timesteps_'+target+'_'+station_name +'_'+fecha+'.joblib'\n",
        "  metric_results_list = []\n",
        "  [r2, RMSE, MAE, MAPE] = RFR_grid(X,y, station, fecha, best_model_dir, best_param_dir, time_steps, time_future, results_dir)\n",
        "  utc_now = datetime.now(pytz.utc)\n",
        "  mexico_city_tz = pytz.timezone('America/Mexico_City')\n",
        "  mexico_city_now = utc_now.astimezone(mexico_city_tz)\n",
        "  hora_formateada = mexico_city_now.strftime('%Y-%m-%d %H:%M')\n",
        "  metric_results_list.append(['RFR', 'NEAREST_STREET', target,station_name, r2, RMSE, MAE, MAPE, len(X), fecha, time_steps, time_future, hora_formateada])\n",
        "  metric_results_df = pd.DataFrame(metric_results_list, columns=column_names)\n",
        "  last_metrics_result = pd.read_csv(dir_metrics_result)\n",
        "  all_metrics = pd.concat([last_metrics_result, metric_results_df])\n",
        "  all_metrics.to_csv(dir_metrics_result, index=False)\n",
        "  results_dir = dir_base+'models_sequence/MODELS_air_pollution_AND_traffic_data/results_XGB_'+'timesteps_'+target+'_'+str(time_future)+'_'+'_'+station_name +'_'+fecha+'.pkl'\n",
        "  best_xgb_model_dir = dir_base+'models_sequence/MODELS_air_pollution_AND_traffic_data/best_model_XGBoost_'+str(time_steps)+'timesteps_'+target+'_'+station_name +'_'+fecha+'.pkl'\n",
        "  best_xgb_params_dir = dir_base+'models_sequence/MODELS_air_pollution_AND_traffic_data/best_paramas_XGBoost_'+str(time_steps)+'timesteps_'+target+'_'+station_name +'_'+fecha+'.pkl'\n",
        "  [r2, RMSE, MAE, MAPE] = XGBoost_grid(X,y, station, fecha, best_xgb_model_dir, best_xgb_params_dir, time_steps, time_future, results_dir)\n",
        "  utc_now = datetime.now(pytz.utc)\n",
        "  mexico_city_tz = pytz.timezone('America/Mexico_City')\n",
        "  mexico_city_now = utc_now.astimezone(mexico_city_tz)\n",
        "  hora_formateada = mexico_city_now.strftime('%Y-%m-%d %H:%M')\n",
        "  metric_results_list = []\n",
        "  metric_results_list.append(['XGBoost', 'NEAREST_STREET', target,station_name, r2, RMSE, MAE, MAPE, len(X), fecha, time_steps, time_future, hora_formateada])\n",
        "  metric_results_df = pd.DataFrame(metric_results_list, columns=column_names)\n",
        "  last_metrics_result = pd.read_csv(dir_metrics_result)\n",
        "  all_metrics = pd.concat([last_metrics_result, metric_results_df])\n",
        "  all_metrics.to_csv(dir_metrics_result, index=False)\n",
        "  results_dir = dir_base+'models_sequence/MODELS_air_pollution_AND_traffic_data/results_RNN_'+'timesteps_'+target+'_'+str(time_future)+'_'+'_'+station_name +'_'+fecha+'.pkl'\n",
        "  RNN_dir = dir_base+'models_sequence/MODELS_air_pollution_AND_traffic_data/best_model_RNN_'+str(time_steps)+'timesteps_'+target+'_'+station_name+'_'+fecha+'.h5'\n",
        "  #[r2, RMSE, MAE, MAPE] = RNN_grid(X,y, station, time_steps, fecha, RNN_dir, time_future, results_dir)\n",
        "  #utc_now = datetime.now(pytz.utc)\n",
        "  #mexico_city_tz = pytz.timezone('America/Mexico_City')\n",
        "  #mexico_city_now = utc_now.astimezone(mexico_city_tz)\n",
        "  #hora_formateada = mexico_city_now.strftime('%Y-%m-%d %H:%M')\n",
        "  #metric_results_list = []\n",
        "  #metric_results_list.append(['RNN', 'NEAREST_STREET', target,station_name, r2, RMSE, MAE, MAPE, len(X), fecha, time_future, hora_formateada])\n",
        "  #metric_results_df = pd.DataFrame(metric_results_list, columns=column_names)\n",
        "  #last_metrics_result = pd.read_csv(dir_metrics_result)\n",
        "  #all_metrics = pd.concat([last_metrics_result, metric_results_df])\n",
        "  #all_metrics.to_csv(dir_metrics_result, index=False)\n",
        "  print(all_metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXIo3El7r-qQ"
      },
      "source": [
        "## Ahora entrenamos los mismos modelos pero sin los datos del tráfico"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 475
        },
        "id": "kGYWnxKFrrH1",
        "outputId": "957d2a75-14ed-428f-bc71-7d66137f8778"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['air_traffic_MER.csv', 'air_traffic_UIZ.csv']\n",
            "/content/drive/MyDrive/docto/A24/Tesis A24/Datos/air_pollutionANDtraffic_data/air_traffic_MER.csv\n",
            "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  pid = os.fork()\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-3f76382a0432>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0mmetric_results_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m   \u001b[0;34m[\u001b[0m\u001b[0mr2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRMSE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAPE\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRFR_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfecha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_model_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_param_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_future\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m   \u001b[0mutc_now\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpytz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m   \u001b[0mmexico_city_tz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpytz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimezone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'America/Mexico_City'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-22e3a0c98bc0>\u001b[0m in \u001b[0;36mRFR_grid\u001b[0;34m(X, y, station_name, fecha, best_model_dir, best_param_dir, time_steps, time_future, results_dir)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m   \u001b[0;31m# Ajustar GridSearchCV a los datos de entrenamiento\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m   \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m   \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    872\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1386\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    819\u001b[0m                     )\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    822\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2005\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2007\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1760\u001b[0m                 (self._jobs[0].get_status(\n\u001b[1;32m   1761\u001b[0m                     timeout=self.timeout) == TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "dir = '/content/drive/MyDrive/docto/A24/Tesis A24/Datos/air_pollutionANDtraffic_data'\n",
        "dir_base = '/content/drive/MyDrive/docto/A24/Tesis A24/Modelos/'\n",
        "dir_metrics_result = dir_base + 'models_sequence/metric_results.csv'\n",
        "files = os.listdir(dir)\n",
        "print(files)\n",
        "metric_results_list= []\n",
        "column_names = ['MODEL', 'TRAFFIC_DATA','TARGET', 'STATION','R2', 'RMSE', 'MAE', 'MAPE', 'LEN_X', 'DATE','TIME_STEPS', 'TIME_FUTURE','HORA']\n",
        "for station in files:\n",
        "  target = \"CO\"\n",
        "  X, y, df, file_path = leer_archivo(station, target)\n",
        "  X = X.drop(columns=['traffic'])\n",
        "  print(file_path)\n",
        "  station_name = station[12:-4]\n",
        "  fecha = '06-06-2024'\n",
        "  time_steps = 12\n",
        "  time_future = 1\n",
        "  \"\"\"\n",
        "  results_dir = dir_base+'models_sequence/MODELS_air_pollution/results_MLP_'+'timesteps_'+target+'_'+str(time_future)+'_'+'_'+station_name +'_'+fecha+'.pkl'\n",
        "  best_model_dir = dir_base+'models_sequence/MODELS_air_pollution/best_model_SEQUENCE_MLP_'+str(time_steps)+'timesteps_'+target+'_'+station_name +'_'+fecha+'.joblib'\n",
        "  best_param_dir = dir_base+'models_sequence/MODELS_air_pollution/best_params_MLP_'+target+'_'+station_name +'_'+fecha+'.joblib'\n",
        "  [r2, RMSE, MAE, MAPE] = MLP_grid(X,y, station_name, fecha, best_model_dir, best_param_dir, time_steps, time_future, results_dir)\n",
        "  utc_now = datetime.now(pytz.utc)\n",
        "  mexico_city_tz = pytz.timezone('America/Mexico_City')\n",
        "  mexico_city_now = utc_now.astimezone(mexico_city_tz)\n",
        "  hora_formateada = mexico_city_now.strftime('%Y-%m-%d %H:%M')\n",
        "  metric_results_list = []\n",
        "  metric_results_list.append(['MLP', 'NO', target,station_name, r2, RMSE, MAE, MAPE, len(X), fecha, time_steps, time_future, hora_formateada])\n",
        "  metric_results_df = pd.DataFrame(metric_results_list, columns=column_names)\n",
        "  last_metrics_result = pd.read_csv(dir_metrics_result)\n",
        "  all_metrics = pd.concat([last_metrics_result, metric_results_df])\n",
        "  all_metrics.to_csv(dir_metrics_result, index=False)\n",
        "  \"\"\"\n",
        "  results_dir = dir_base+'models_sequence/MODELS_air_pollution/results_RFR_'+'timesteps_'+target+'_'+str(time_future)+'_'+'_'+station_name +'_'+fecha+'.pkl'\n",
        "  best_model_dir = dir_base+'models_sequence/MODELS_air_pollution/best_model_RFR_'+str(time_steps)+'timesteps_'+target+'_'+station_name +'_'+fecha+'.joblib'\n",
        "  best_param_dir = dir_base+'models_sequence/MODELS_air_pollution/best_params_RFR_'+str(time_steps)+'timesteps_'+target+'_'+station_name +'_'+fecha+'.joblib'\n",
        "\n",
        "  metric_results_list = []\n",
        "  [r2, RMSE, MAE, MAPE] = RFR_grid(X,y, station, fecha, best_model_dir, best_param_dir, time_steps, time_future, results_dir)\n",
        "  utc_now = datetime.now(pytz.utc)\n",
        "  mexico_city_tz = pytz.timezone('America/Mexico_City')\n",
        "  mexico_city_now = utc_now.astimezone(mexico_city_tz)\n",
        "  hora_formateada = mexico_city_now.strftime('%Y-%m-%d %H:%M')\n",
        "  metric_results_list.append(['RFR', 'NO', target,station_name, r2, RMSE, MAE, MAPE, len(X), fecha, time_steps, time_future, hora_formateada])\n",
        "  metric_results_df = pd.DataFrame(metric_results_list, columns=column_names)\n",
        "  last_metrics_result = pd.read_csv(dir_metrics_result)\n",
        "  all_metrics = pd.concat([last_metrics_result, metric_results_df])\n",
        "  all_metrics.to_csv(dir_metrics_result, index=False)\n",
        "\n",
        "  results_dir = dir_base+'models_sequence/MODELS_air_pollution/results_XGB_'+'timesteps_'+target+'_'+str(time_future)+'_'+'_'+station_name +'_'+fecha+'.pkl'\n",
        "  best_xgb_model_dir = dir_base+'models_sequence/MODELS_air_pollution/best_model_XGBoost_'+str(time_steps)+'timesteps_'+target+'_'+station_name +'_'+fecha+'.pkl'\n",
        "  best_xgb_params_dir = dir_base+'models_sequence/MODELS_air_pollution/best_paramas_XGBoost_'+str(time_steps)+'timesteps_'+target+'_'+station_name +'_'+fecha+'.pkl'\n",
        "  [r2, RMSE, MAE, MAPE] = XGBoost_grid(X,y, station, fecha, best_xgb_model_dir, best_xgb_params_dir, time_steps, time_future, results_dir)\n",
        "  utc_now = datetime.now(pytz.utc)\n",
        "  mexico_city_tz = pytz.timezone('America/Mexico_City')\n",
        "  mexico_city_now = utc_now.astimezone(mexico_city_tz)\n",
        "  hora_formateada = mexico_city_now.strftime('%Y-%m-%d %H:%M')\n",
        "  metric_results_list = []\n",
        "  metric_results_list.append(['XGBoost', 'NO', target,station_name, r2, RMSE, MAE, MAPE, len(X), fecha, time_steps, time_future, hora_formateada])\n",
        "  metric_results_df = pd.DataFrame(metric_results_list, columns=column_names)\n",
        "  last_metrics_result = pd.read_csv(dir_metrics_result)\n",
        "  all_metrics = pd.concat([last_metrics_result, metric_results_df])\n",
        "  all_metrics.to_csv(dir_metrics_result, index=False)\n",
        "  results_dir = dir_base+'models_sequence/MODELS_air_pollution/results_RNN_'+'timesteps_'+target+'_'+str(time_future)+'_'+'_'+station_name +'_'+fecha+'.pkl'\n",
        "  RNN_dir = dir_base+'models_sequence/MODELS_air_pollution/best_model_RNN_'+str(time_steps)+'timesteps_'+target+'_'+station_name+'_'+fecha+'.h5'\n",
        "  #[r2, RMSE, MAE, MAPE] = RNN_grid(X,y, station, time_steps, fecha, RNN_dir, time_future, results_dir)\n",
        "  #utc_now = datetime.now(pytz.utc)\n",
        "  #mexico_city_tz = pytz.timezone('America/Mexico_City')\n",
        "  #mexico_city_now = utc_now.astimezone(mexico_city_tz)\n",
        "  #hora_formateada = mexico_city_now.strftime('%Y-%m-%d %H:%M')\n",
        "  #metric_results_list = []\n",
        "  #metric_results_list.append(['RNN', 'NO', target,station_name, r2, RMSE, MAE, MAPE, len(X), fecha, time_future, hora_formateada])\n",
        "  #metric_results_df = pd.DataFrame(metric_results_list, columns=column_names)\n",
        "  #last_metrics_result = pd.read_csv(dir_metrics_result)\n",
        "  #all_metrics = pd.concat([last_metrics_result, metric_results_df])\n",
        "  #all_metrics.to_csv(dir_metrics_result, index=False)\n",
        "\n",
        "  print(all_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "dir = '/content/drive/MyDrive/docto/A24/Tesis A24/Modelos/models_sequence/MODELS_air_pollution_AND_traffic_data/results_XGB_CO_UIZ_06-06-2024.pkl'\n",
        "# Cargar los resultados desde el archivo\n",
        "with open(dir, 'rb') as f:\n",
        "    loaded_results = pickle.load(f)\n",
        "\n",
        "# Mostrar los resultados de los puntajes de entrenamiento\n",
        "train_scores = loaded_results['mean_train_score']\n",
        "print(\"Mean train scores for each parameter combination:\")\n",
        "for mean_train_score in train_scores:\n",
        "    print(mean_train_score)\n"
      ],
      "metadata": {
        "id": "UnD-xYK8TFAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [3, 4, 5],\n",
        "    'learning_rate': [0.01, 0.05, 0.1]\n",
        "}\n",
        "\n",
        "# Cargar un conjunto de datos de ejemplo\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Definir un modelo\n",
        "model = RandomForestClassifier()\n",
        "\n",
        "# Definir una rejilla de parámetros\n",
        "param_grid = {\n",
        "    'n_estimators': [10, 50, 100],\n",
        "    'max_depth': [1, 10, 20]\n",
        "}\n",
        "\n",
        "# Crear el objeto GridSearchCV con return_train_score=True\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, return_train_score=True)\n",
        "\n",
        "# Ajustar el modelo a los datos\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Obtener los resultados\n",
        "results = grid_search.cv_results_\n",
        "\n",
        "#results = loaded_results\n",
        "# Extraer los puntajes de entrenamiento y validación\n",
        "train_scores = np.array(results['mean_train_score']).reshape(len(param_grid['max_depth']), len(param_grid['n_estimators']))\n",
        "test_scores = np.array(results['mean_test_score']).reshape(len(param_grid['max_depth']), len(param_grid['n_estimators']))\n",
        "\n",
        "# Graficar los resultados\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# Crear rejilla de parámetros\n",
        "n_estimators, max_depth = np.meshgrid(param_grid['n_estimators'], param_grid['max_depth'])\n",
        "\n",
        "# Graficar superficies\n",
        "ax.plot_surface(n_estimators, max_depth, train_scores, cmap='viridis', label='Train Score')\n",
        "ax.plot_surface(n_estimators, max_depth, test_scores, cmap='plasma', label='Test Score')\n",
        "\n",
        "# Etiquetas y título\n",
        "ax.set_xlabel('Number of Estimators')\n",
        "ax.set_ylabel('Max Depth')\n",
        "ax.set_zlabel('Mean Score')\n",
        "ax.set_title('Grid Search Results')\n",
        "\n",
        "# Añadir leyenda\n",
        "ax.legend()\n",
        "\n",
        "# Mostrar la gráfica\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dyzL_tWnTZWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "id": "DWqurJ4UTjEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6G0oZAW3UNBH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}