{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SuAkpPKsW9UQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "nuevo_nombre_carpeta = 'all_data_2005xstation_15_05_2024'  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASeowmTGXT92"
      },
      "source": [
        "Primero unimos los nuevos datos a los datos ya guardados en su respectiva estación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#Valores hasta 15 de mayo 2024\n",
        "AJM.csv AJM.csv\n",
        "108873 1064 109937\n",
        "BJU.csv BJU.csv\n",
        "121613 1065 122678\n",
        "CAM.csv CAM.csv\n",
        "217118 1014 218132\n",
        "CCA.csv CCA.csv\n",
        "133365 1053 134418\n",
        "CUA.csv CUA.csv\n",
        "228566 1042 229608\n",
        "GAM.csv GAM.csv\n",
        "109108 1064 110172\n",
        "MGH.csv MGH.csv\n",
        "116118 1063 117181\n",
        "NEZ.csv NEZ.csv\n",
        "194751 1058 195809\n",
        "PED.csv PED.csv\n",
        "254680 992 255672\n",
        "SAC.csv SAC.csv\n",
        "56018 1066 57084\n",
        "SAG.csv SAG.csv\n",
        "247532 931 248463\n",
        "SFE.csv SFE.csv\n",
        "134231 0 134231\n",
        "TAH.csv TAH.csv\n",
        "228177 1052 229229\n",
        "TLI.csv TLI.csv\n",
        "226817 1058 227875\n",
        "VIF.csv VIF.csv\n",
        "234964 1010 235974"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LK24Q0OfXbtN",
        "outputId": "56efc8c5-e1b9-4b2a-bc6c-16a7ae000288"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AJM.csv AJM.csv\n",
            "108873 1064 109937\n",
            "BJU.csv BJU.csv\n",
            "121613 1065 122678\n",
            "CAM.csv CAM.csv\n",
            "217118 1014 218132\n",
            "CCA.csv CCA.csv\n",
            "133365 1053 134418\n",
            "CUA.csv CUA.csv\n",
            "228566 1042 229608\n",
            "GAM.csv GAM.csv\n",
            "109108 1064 110172\n",
            "MGH.csv MGH.csv\n",
            "116118 1063 117181\n",
            "NEZ.csv NEZ.csv\n",
            "194751 1058 195809\n",
            "PED.csv PED.csv\n",
            "254680 992 255672\n",
            "SAC.csv SAC.csv\n",
            "56018 1066 57084\n",
            "SAG.csv SAG.csv\n",
            "247532 931 248463\n",
            "SFE.csv SFE.csv\n",
            "134231 0 134231\n",
            "TAH.csv TAH.csv\n",
            "228177 1052 229229\n",
            "TLI.csv TLI.csv\n",
            "226817 1058 227875\n",
            "VIF.csv VIF.csv\n",
            "234964 1010 235974\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Tener cuidado, descomentar y comentar para ejecutar SOLO 1 vez\n",
        "\"\"\"\n",
        "dir = \"all_data_2005xstation_31_03_2024\"\n",
        "dirNewData = \"datos_por_estacion\"\n",
        "filesPREV = os.listdir(dir)\n",
        "filesNEW = os.listdir(dirNewData)\n",
        "for file_name in filesPREV:\n",
        "  for file_nameNEW in filesNEW:\n",
        "    if file_name == file_nameNEW:\n",
        "      print(file_name, file_nameNEW)\n",
        "      file_pathPREV = os.path.join(dir,file_name)\n",
        "      datasetPREV = pd.read_csv(file_pathPREV)\n",
        "      file_pathNEWdata = os.path.join(dirNewData,file_name)\n",
        "      datasetNEWdata = pd.read_csv(file_pathNEWdata)\n",
        "      new_df = pd.concat([datasetPREV, datasetNEWdata])\n",
        "      new_df.to_csv(file_pathPREV, index=False)\n",
        "      print(len(datasetPREV), len(datasetNEWdata), len(new_df))\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se renombra la carpeta con la fecha de los últimos datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Renombrar la carpeta\n",
        "os.rename(dir, nuevo_nombre_carpeta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTmIFPiiaolP"
      },
      "source": [
        "Luego los segmentamos en 80%, 15% y 5% para entrenamiento, validación y prueba respectivamente\n",
        "(train valid test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gAd2eJzXKFJ",
        "outputId": "47028060-e615-4754-a0b1-2778159de157"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "entrenamiento_datos_por_estacion\\AJM.csv 109937 87950 87950 80 5497 5 16490 15.0 25.45 25.77 40.44 18.77\n",
            "entrenamiento_datos_por_estacion\\BJU.csv 122678 98142 98142 80 6134 5 18402 15.0 32.91 33.35 44.31 26.75\n",
            "entrenamiento_datos_por_estacion\\CAM.csv 218132 174506 174506 80 10907 5 32719 15.0 40.68 41.91 33.63 36.42\n",
            "entrenamiento_datos_por_estacion\\CCA.csv 134418 107534 107534 80 6721 5 20163 15.0 36.57 36.68 36.47 36.07\n",
            "entrenamiento_datos_por_estacion\\CUA.csv 229608 183686 183686 80 11480 5 34442 15.0 32.69 33.71 31.45 27.68\n",
            "entrenamiento_datos_por_estacion\\GAM.csv 110172 88138 88138 80 5509 5 16525 15.0 39.18 38.8 52.39 36.78\n",
            "entrenamiento_datos_por_estacion\\MGH.csv 117181 93745 93745 80 5859 5 17577 15.0 26.87 25.98 46.37 25.14\n",
            "entrenamiento_datos_por_estacion\\NEZ.csv 195809 156647 156647 80 9790 5 29372 15.0 32.99 34.12 33.42 26.78\n",
            "entrenamiento_datos_por_estacion\\PED.csv 255672 204538 204538 80 12784 5 38350 15.0 25.76 26.22 25.09 23.53\n",
            "entrenamiento_datos_por_estacion\\SAC.csv 57084 45667 45667 80 2854 5 8563 15.0 29.37 30.5 23.45 25.36\n",
            "entrenamiento_datos_por_estacion\\SAG.csv 248463 198770 198770 80 12423 5 37270 15.0 25.4 25.1 23.81 27.52\n",
            "entrenamiento_datos_por_estacion\\SFE.csv 134231 107385 107385 80 6712 5 20134 15.0 26.97 27.78 5.38 29.82\n",
            "entrenamiento_datos_por_estacion\\TAH.csv 229229 183383 183383 80 11461 5 34385 15.0 34.06 35.57 23.18 29.59\n",
            "entrenamiento_datos_por_estacion\\TLI.csv 227875 182300 182300 80 11394 5 34181 15.0 39.46 39.24 42.73 39.52\n",
            "entrenamiento_datos_por_estacion\\VIF.csv 235974 188779 188779 80 11799 5 35396 15.0 27.5 27.1 20.34 32.04\n"
          ]
        }
      ],
      "source": [
        "dir = nuevo_nombre_carpeta\n",
        "new_dirTest = \"prueba_datos_por_estacion\"\n",
        "new_dirTrain = \"entrenamiento_datos_por_estacion\"\n",
        "new_dirVal = \"validacion_datos_por_estacion\"\n",
        "\n",
        "if not os.path.exists(new_dirTest):\n",
        "    os.makedirs(new_dirTest)\n",
        "    \n",
        "if not os.path.exists(new_dirTrain):\n",
        "    os.makedirs(new_dirTrain)\n",
        "    \n",
        "if not os.path.exists(new_dirVal):\n",
        "    os.makedirs(new_dirVal)\n",
        "\n",
        "info_datasets = []\n",
        "columnas = [\"file_name\", \"len_dataset\",\".8dataset\", \"len_trainSet\", \"train%\", \"len_testSet\", \"test%\", \"lenValSet\", \"val%\", \"%datosFaltantes\", \"%dastosFaltantesTrain\", \"%dastosFaltantesTest\", \"%dastosFaltantesVal\"]\n",
        "files = os.listdir(dir)\n",
        "for file_name in files:\n",
        "  file_path = os.path.join(dir,file_name)\n",
        "  dataset = pd.read_csv(file_path)\n",
        "  dataset[['year', 'month', 'day']] = dataset['date'].str.split('/', expand=True)\n",
        "  dataset[\"hour\"] = dataset[\"day\"]\n",
        "  dataset[[\"day\", \"hour\"]] = dataset[\"day\"].str.split(' ', expand=True)\n",
        "  dataset[[\"hour\",\"minute\"]] = dataset[\"hour\"].str.split(':', expand=True)\n",
        "  train_procentage = round(len(dataset)*.8)\n",
        "  validation_porcentage = round(len(dataset)*.15)\n",
        "  test_porcentage = round(len(dataset)*.05)\n",
        "  train_set = dataset[0:train_procentage]\n",
        "  test_set = dataset[train_procentage:train_procentage+test_porcentage]\n",
        "  val_set = dataset[train_procentage+test_porcentage:len(dataset)]\n",
        "  porcentaje_total_vacios = round(dataset.isna().mean().mean() * 100,2)\n",
        "  porc_vacios_train = round(train_set.isna().mean().mean() * 100,2)\n",
        "  porc_vacios_test = round(test_set.isna().mean().mean() * 100,2)\n",
        "  porc_vacios_val = round(val_set.isna().mean().mean() * 100,2)\n",
        "  info_datasets.append([file_name, len(dataset), round(len(dataset)*.8), len(train_set), round(len(train_set)*100/len(dataset)), len(test_set), round(len(test_set)*100/len(dataset)),len(val_set), round((len(val_set)*100/len(dataset)),2), porcentaje_total_vacios, porc_vacios_train, porc_vacios_test, porc_vacios_val])\n",
        "  new_nameTrain = os.path.join(new_dirTrain, file_name)\n",
        "  new_nameTest = os.path.join(new_dirTest, file_name)\n",
        "  new_nameVal = os.path.join(new_dirVal, file_name)\n",
        "  train_set.to_csv(new_nameTrain, index=False)\n",
        "  test_set.to_csv(new_nameTest, index=False)\n",
        "  val_set.to_csv(new_nameVal, index=False)\n",
        "  print(new_nameTrain, len(dataset), round(len(dataset)*.8), len(train_set), round(len(train_set)*100/len(dataset)), len(test_set), round(len(test_set)*100/len(dataset)),len(val_set), round((len(val_set)*100/len(dataset)),2), porcentaje_total_vacios, porc_vacios_train, porc_vacios_test, porc_vacios_val)\n",
        "\n",
        "info_name = \"info_split_datasets.csv\"\n",
        "info_datasets = pd.DataFrame(info_datasets, columns=columnas)\n",
        "info_datasets.to_csv(info_name, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Las carpetas con los datos x estación ya segmentados se encuentran en entrenamiento / prueba / y validación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se crean nuevas carpetas con archivos sin datos faltantes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "entrenamiento_datos_por_estacion\\AJM.csv\n",
            "entrenamiento_datos_por_estacion\\BJU.csv\n",
            "entrenamiento_datos_por_estacion\\CAM.csv\n",
            "entrenamiento_datos_por_estacion\\CCA.csv\n",
            "entrenamiento_datos_por_estacion\\CUA.csv\n",
            "entrenamiento_datos_por_estacion\\GAM.csv\n",
            "entrenamiento_datos_por_estacion\\MGH.csv\n",
            "entrenamiento_datos_por_estacion\\NEZ.csv\n",
            "entrenamiento_datos_por_estacion\\PED.csv\n",
            "entrenamiento_datos_por_estacion\\SAC.csv\n",
            "entrenamiento_datos_por_estacion\\SAG.csv\n",
            "entrenamiento_datos_por_estacion\\SFE.csv\n",
            "entrenamiento_datos_por_estacion\\TAH.csv\n",
            "entrenamiento_datos_por_estacion\\TLI.csv\n",
            "entrenamiento_datos_por_estacion\\VIF.csv\n",
            "validacion_datos_por_estacion\\AJM.csv\n",
            "validacion_datos_por_estacion\\BJU.csv\n",
            "validacion_datos_por_estacion\\CAM.csv\n",
            "validacion_datos_por_estacion\\CCA.csv\n",
            "validacion_datos_por_estacion\\CUA.csv\n",
            "validacion_datos_por_estacion\\GAM.csv\n",
            "validacion_datos_por_estacion\\MGH.csv\n",
            "validacion_datos_por_estacion\\NEZ.csv\n",
            "validacion_datos_por_estacion\\PED.csv\n",
            "validacion_datos_por_estacion\\SAC.csv\n",
            "validacion_datos_por_estacion\\SAG.csv\n",
            "validacion_datos_por_estacion\\SFE.csv\n",
            "validacion_datos_por_estacion\\TAH.csv\n",
            "validacion_datos_por_estacion\\TLI.csv\n",
            "validacion_datos_por_estacion\\VIF.csv\n",
            "prueba_datos_por_estacion\\AJM.csv\n",
            "prueba_datos_por_estacion\\BJU.csv\n",
            "prueba_datos_por_estacion\\CAM.csv\n",
            "prueba_datos_por_estacion\\CCA.csv\n",
            "prueba_datos_por_estacion\\CUA.csv\n",
            "prueba_datos_por_estacion\\GAM.csv\n",
            "prueba_datos_por_estacion\\MGH.csv\n",
            "prueba_datos_por_estacion\\NEZ.csv\n",
            "prueba_datos_por_estacion\\PED.csv\n",
            "prueba_datos_por_estacion\\SAC.csv\n",
            "prueba_datos_por_estacion\\SAG.csv\n",
            "prueba_datos_por_estacion\\SFE.csv\n",
            "prueba_datos_por_estacion\\TAH.csv\n",
            "prueba_datos_por_estacion\\TLI.csv\n",
            "prueba_datos_por_estacion\\VIF.csv\n"
          ]
        }
      ],
      "source": [
        "dir = \"entrenamiento_datos_por_estacion\"\n",
        "dir_sinNaN = \"entrenamiento_datos_por_estacion_sin_NaN\"\n",
        "if not os.path.exists(dir_sinNaN ):\n",
        "    os.makedirs(dir_sinNaN )\n",
        "files = os.listdir(dir)\n",
        "for file_name in files:\n",
        "  file_path = os.path.join(dir,file_name)\n",
        "  print(file_path)\n",
        "  dataset = pd.read_csv(file_path)\n",
        "  dataset = dataset.dropna(axis=1, how='all')\n",
        "  dataset = dataset.dropna(axis=0)\n",
        "  df = dataset[[\"date\", \"year\", \"month\", \"day\", \"hour\", \"minute\"]]\n",
        "  dataset = dataset.drop([\"date\", \"year\", \"month\", \"day\", \"hour\", \"minute\"], axis=1)\n",
        "  for column in dataset.columns:\n",
        "    if dataset[column].nunique() == 1:\n",
        "      dataset = dataset.drop(columns=[column], axis=1)\n",
        "  dataset.insert(0, \"date\", df[\"date\"])\n",
        "  df = df.drop([\"date\"], axis=1)\n",
        "  dataset[[\"year\", \"month\", \"day\", \"hour\", \"minute\"]] = df\n",
        "  name = os.path.join(dir_sinNaN, file_name)\n",
        "  dataset.to_csv(name, index=False)\n",
        "\n",
        "dir = \"validacion_datos_por_estacion\"\n",
        "dir_sinNaN = \"validacion_datos_por_estacion_sin_NaN\"\n",
        "if not os.path.exists(dir_sinNaN):\n",
        "    os.makedirs(dir_sinNaN )\n",
        "files = os.listdir(dir)\n",
        "for file_name in files:\n",
        "  file_path = os.path.join(dir,file_name)\n",
        "  print(file_path)\n",
        "  dataset = pd.read_csv(file_path)\n",
        "  dataset = dataset.dropna(axis=1, how='all')\n",
        "  dataset = dataset.dropna(axis=0)\n",
        "  df = dataset[[\"date\", \"year\", \"month\", \"day\", \"hour\", \"minute\"]]\n",
        "  dataset = dataset.drop([\"date\", \"year\", \"month\", \"day\", \"hour\", \"minute\"], axis=1)\n",
        "  for column in dataset.columns:\n",
        "    if dataset[column].nunique() == 1:\n",
        "      dataset = dataset.drop(columns=[column], axis=1)\n",
        "  dataset.insert(0, \"date\", df[\"date\"])\n",
        "  df = df.drop([\"date\"], axis=1)\n",
        "  dataset[[\"year\", \"month\", \"day\", \"hour\", \"minute\"]] = df\n",
        "  name = os.path.join(dir_sinNaN, file_name)\n",
        "  dataset.to_csv(name, index=False)\n",
        "\n",
        "\n",
        "dir = \"prueba_datos_por_estacion\"\n",
        "dir_sinNaN = \"prueba_datos_por_estacion_sin_NaN\"\n",
        "if not os.path.exists(dir_sinNaN):\n",
        "    os.makedirs(dir_sinNaN )\n",
        "files = os.listdir(dir)\n",
        "for file_name in files:\n",
        "  file_path = os.path.join(dir,file_name)\n",
        "  print(file_path)\n",
        "  dataset = pd.read_csv(file_path)\n",
        "  dataset = dataset.dropna(axis=1, how='all')\n",
        "  dataset = dataset.dropna(axis=0)\n",
        "  df = dataset[[\"date\", \"year\", \"month\", \"day\", \"hour\", \"minute\"]]\n",
        "  dataset = dataset.drop([\"date\", \"year\", \"month\", \"day\", \"hour\", \"minute\"], axis=1)\n",
        "  for column in dataset.columns:\n",
        "    if dataset[column].nunique() == 1:\n",
        "      dataset = dataset.drop(columns=[column], axis=1)\n",
        "  dataset.insert(0, \"date\", df[\"date\"])\n",
        "  df = df.drop([\"date\"], axis=1)\n",
        "  dataset[[\"year\", \"month\", \"day\", \"hour\", \"minute\"]] = df\n",
        "  name = os.path.join(dir_sinNaN, file_name)\n",
        "  dataset.to_csv(name, index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
