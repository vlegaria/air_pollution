{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SuAkpPKsW9UQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "nuevo_nombre_carpeta = 'all_data_2005xstation_15_05_2024'  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASeowmTGXT92"
      },
      "source": [
        "Primero unimos los nuevos datos a los datos ya guardados en su respectiva estación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#Valores hasta 15 de mayo 2024\n",
        "AJM.csv AJM.csv\n",
        "108873 1064 109937\n",
        "BJU.csv BJU.csv\n",
        "121613 1065 122678\n",
        "CAM.csv CAM.csv\n",
        "217118 1014 218132\n",
        "CCA.csv CCA.csv\n",
        "133365 1053 134418\n",
        "CUA.csv CUA.csv\n",
        "228566 1042 229608\n",
        "GAM.csv GAM.csv\n",
        "109108 1064 110172\n",
        "MGH.csv MGH.csv\n",
        "116118 1063 117181\n",
        "NEZ.csv NEZ.csv\n",
        "194751 1058 195809\n",
        "PED.csv PED.csv\n",
        "254680 992 255672\n",
        "SAC.csv SAC.csv\n",
        "56018 1066 57084\n",
        "SAG.csv SAG.csv\n",
        "247532 931 248463\n",
        "SFE.csv SFE.csv\n",
        "134231 0 134231\n",
        "TAH.csv TAH.csv\n",
        "228177 1052 229229\n",
        "TLI.csv TLI.csv\n",
        "226817 1058 227875\n",
        "VIF.csv VIF.csv\n",
        "234964 1010 235974"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LK24Q0OfXbtN",
        "outputId": "56efc8c5-e1b9-4b2a-bc6c-16a7ae000288"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BJU.csv BJU.csv\n",
            "120894 1784 122678\n",
            "FAC.csv FAC.csv\n",
            "268964 1773 270737\n",
            "NEZ.csv NEZ.csv\n",
            "194007 1802 195809\n",
            "PED.csv PED.csv\n",
            "253982 1690 255672\n",
            "SAC.csv SAC.csv\n",
            "55274 1810 57084\n"
          ]
        }
      ],
      "source": [
        "# Tener cuidado, descomentar y comentar para ejecutar SOLO 1 vez\n",
        "\"\"\"\n",
        "dir = \"all_data_2005xstation_31_03_2024\"\n",
        "dirNewData = \"datos_por_estacion\"\n",
        "filesPREV = os.listdir(dir)\n",
        "filesNEW = os.listdir(dirNewData)\n",
        "for file_name in filesPREV:\n",
        "  for file_nameNEW in filesNEW:\n",
        "    if file_name == file_nameNEW:\n",
        "      print(file_name, file_nameNEW)\n",
        "      file_pathPREV = os.path.join(dir,file_name)\n",
        "      datasetPREV = pd.read_csv(file_pathPREV)\n",
        "      file_pathNEWdata = os.path.join(dirNewData,file_name)\n",
        "      datasetNEWdata = pd.read_csv(file_pathNEWdata)\n",
        "      new_df = pd.concat([datasetPREV, datasetNEWdata])\n",
        "      new_df.to_csv(file_pathPREV, index=False)\n",
        "      print(len(datasetPREV), len(datasetNEWdata), len(new_df))\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se renombra la carpeta con la fecha de los últimos datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Renombrar la carpeta\n",
        "os.rename(dir, nuevo_nombre_carpeta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "all_data_2005xstation_15_05_2024\\ACO.csv\n",
            "all_data_2005xstation_15_05_2024\\AJM.csv\n",
            "all_data_2005xstation_15_05_2024\\AJU.csv\n",
            "all_data_2005xstation_15_05_2024\\ARA.csv\n",
            "all_data_2005xstation_15_05_2024\\ATI.csv\n",
            "all_data_2005xstation_15_05_2024\\AZC.csv\n",
            "all_data_2005xstation_15_05_2024\\BJU.csv\n",
            "all_data_2005xstation_15_05_2024\\CAM.csv\n",
            "all_data_2005xstation_15_05_2024\\CCA.csv\n",
            "all_data_2005xstation_15_05_2024\\CES.csv\n",
            "all_data_2005xstation_15_05_2024\\CHO.csv\n",
            "all_data_2005xstation_15_05_2024\\COY.csv\n",
            "all_data_2005xstation_15_05_2024\\CUA.csv\n",
            "all_data_2005xstation_15_05_2024\\CUT.csv\n",
            "all_data_2005xstation_15_05_2024\\FAC.csv\n",
            "all_data_2005xstation_15_05_2024\\FAR.csv\n",
            "all_data_2005xstation_15_05_2024\\GAM.csv\n",
            "all_data_2005xstation_15_05_2024\\HGM.csv\n",
            "all_data_2005xstation_15_05_2024\\IMP.csv\n",
            "all_data_2005xstation_15_05_2024\\INN.csv\n",
            "all_data_2005xstation_15_05_2024\\IZT.csv\n",
            "all_data_2005xstation_15_05_2024\\LAG.csv\n",
            "all_data_2005xstation_15_05_2024\\LLA.csv\n",
            "all_data_2005xstation_15_05_2024\\LPR.csv\n",
            "all_data_2005xstation_15_05_2024\\MER.csv\n",
            "all_data_2005xstation_15_05_2024\\MGH.csv\n",
            "all_data_2005xstation_15_05_2024\\MON.csv\n",
            "all_data_2005xstation_15_05_2024\\MPA.csv\n",
            "all_data_2005xstation_15_05_2024\\NEZ.csv\n",
            "all_data_2005xstation_15_05_2024\\PED.csv\n",
            "all_data_2005xstation_15_05_2024\\PLA.csv\n",
            "all_data_2005xstation_15_05_2024\\SAC.csv\n",
            "all_data_2005xstation_15_05_2024\\SAG.csv\n",
            "all_data_2005xstation_15_05_2024\\SFE.csv\n",
            "all_data_2005xstation_15_05_2024\\SJA.csv\n",
            "all_data_2005xstation_15_05_2024\\SUR.csv\n",
            "all_data_2005xstation_15_05_2024\\TAC.csv\n",
            "all_data_2005xstation_15_05_2024\\TAH.csv\n",
            "all_data_2005xstation_15_05_2024\\TAX.csv\n",
            "all_data_2005xstation_15_05_2024\\TEC.csv\n",
            "all_data_2005xstation_15_05_2024\\TLA.csv\n",
            "all_data_2005xstation_15_05_2024\\TLI.csv\n",
            "all_data_2005xstation_15_05_2024\\TPN.csv\n",
            "all_data_2005xstation_15_05_2024\\UAX.csv\n",
            "all_data_2005xstation_15_05_2024\\UIZ.csv\n",
            "all_data_2005xstation_15_05_2024\\VAL.csv\n",
            "all_data_2005xstation_15_05_2024\\VIF.csv\n",
            "all_data_2005xstation_15_05_2024\\XAL.csv\n"
          ]
        }
      ],
      "source": [
        "dir =  nuevo_nombre_carpeta\n",
        "dir_sinNaN = \"datos_por_estacion_sin_NaN\"\n",
        "if not os.path.exists(dir_sinNaN ):\n",
        "    os.makedirs(dir_sinNaN )\n",
        "files = os.listdir(dir)\n",
        "\n",
        "info_datasets = []\n",
        "columnas = [\"file_name\", \"len_dataset\",\"%datosFaltantes\"]\n",
        "\n",
        "for file_name in files:\n",
        "  file_path = os.path.join(dir,file_name)\n",
        "  print(file_path)\n",
        "  dataset = pd.read_csv(file_path)\n",
        "  dataset[['year', 'month', 'day']] = dataset['date'].str.split('/', expand=True)\n",
        "  dataset[\"hour\"] = dataset[\"day\"]\n",
        "  dataset[[\"day\", \"hour\"]] = dataset[\"day\"].str.split(' ', expand=True)\n",
        "  dataset[[\"hour\",\"minute\"]] = dataset[\"hour\"].str.split(':', expand=True)\n",
        "\n",
        "  porcentaje_total_vacios = round(dataset.isna().mean().mean() * 100,2)\n",
        "  info_datasets.append([file_name, len(dataset), porcentaje_total_vacios])\n",
        "    \n",
        "  dataset = dataset.dropna(axis=1, how='all')\n",
        "  dataset = dataset.dropna(axis=0)\n",
        "  df = dataset[[\"date\", \"year\", \"month\", \"day\", \"hour\", \"minute\"]]\n",
        "  dataset = dataset.drop([\"date\", \"year\", \"month\", \"day\", \"hour\", \"minute\"], axis=1)\n",
        "  for column in dataset.columns:\n",
        "    if dataset[column].nunique() == 1:\n",
        "      dataset = dataset.drop(columns=[column], axis=1)\n",
        "  dataset.insert(0, \"date\", df[\"date\"])\n",
        "  df = df.drop([\"date\"], axis=1)\n",
        "  dataset[[\"year\", \"month\", \"day\", \"hour\", \"minute\"]] = df\n",
        "  name = os.path.join(dir_sinNaN, file_name)\n",
        "  dataset.to_csv(name, index=False)\n",
        "\n",
        "info_name = \"info_split_datasets.csv\"\n",
        "info_datasets = pd.DataFrame(info_datasets, columns=columnas)\n",
        "info_datasets.to_csv(info_name, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "      <th>len_dataset</th>\n",
              "      <th>%datosFaltantes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ACO.csv</td>\n",
              "      <td>193368</td>\n",
              "      <td>33.19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>AJM.csv</td>\n",
              "      <td>109937</td>\n",
              "      <td>25.45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AJU.csv</td>\n",
              "      <td>97827</td>\n",
              "      <td>43.39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ARA.csv</td>\n",
              "      <td>41899</td>\n",
              "      <td>59.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ATI.csv</td>\n",
              "      <td>245317</td>\n",
              "      <td>42.49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>AZC.csv</td>\n",
              "      <td>81505</td>\n",
              "      <td>44.70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>BJU.csv</td>\n",
              "      <td>122678</td>\n",
              "      <td>32.91</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>CAM.csv</td>\n",
              "      <td>218132</td>\n",
              "      <td>40.68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>CCA.csv</td>\n",
              "      <td>134418</td>\n",
              "      <td>36.57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>CES.csv</td>\n",
              "      <td>85108</td>\n",
              "      <td>31.79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>CHO.csv</td>\n",
              "      <td>200908</td>\n",
              "      <td>38.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>COY.csv</td>\n",
              "      <td>144137</td>\n",
              "      <td>48.24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>CUA.csv</td>\n",
              "      <td>229608</td>\n",
              "      <td>32.69</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>CUT.csv</td>\n",
              "      <td>158321</td>\n",
              "      <td>30.33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>FAC.csv</td>\n",
              "      <td>270737</td>\n",
              "      <td>25.72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>FAR.csv</td>\n",
              "      <td>71260</td>\n",
              "      <td>35.63</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>GAM.csv</td>\n",
              "      <td>110172</td>\n",
              "      <td>39.18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>HGM.csv</td>\n",
              "      <td>157736</td>\n",
              "      <td>30.95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>IMP.csv</td>\n",
              "      <td>74286</td>\n",
              "      <td>58.81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>INN.csv</td>\n",
              "      <td>108588</td>\n",
              "      <td>38.54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>IZT.csv</td>\n",
              "      <td>214234</td>\n",
              "      <td>36.63</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>LAG.csv</td>\n",
              "      <td>83993</td>\n",
              "      <td>40.69</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>LLA.csv</td>\n",
              "      <td>169806</td>\n",
              "      <td>50.28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>LPR.csv</td>\n",
              "      <td>207427</td>\n",
              "      <td>52.99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>MER.csv</td>\n",
              "      <td>257049</td>\n",
              "      <td>22.64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>MGH.csv</td>\n",
              "      <td>117181</td>\n",
              "      <td>26.87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>MON.csv</td>\n",
              "      <td>222329</td>\n",
              "      <td>36.41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>MPA.csv</td>\n",
              "      <td>94460</td>\n",
              "      <td>38.07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>NEZ.csv</td>\n",
              "      <td>195809</td>\n",
              "      <td>32.99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>PED.csv</td>\n",
              "      <td>255672</td>\n",
              "      <td>25.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>PLA.csv</td>\n",
              "      <td>82556</td>\n",
              "      <td>28.97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>SAC.csv</td>\n",
              "      <td>57084</td>\n",
              "      <td>29.37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>SAG.csv</td>\n",
              "      <td>248463</td>\n",
              "      <td>25.40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>SFE.csv</td>\n",
              "      <td>134231</td>\n",
              "      <td>26.97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>SJA.csv</td>\n",
              "      <td>124413</td>\n",
              "      <td>45.88</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>SUR.csv</td>\n",
              "      <td>154987</td>\n",
              "      <td>30.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>TAC.csv</td>\n",
              "      <td>76250</td>\n",
              "      <td>29.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>TAH.csv</td>\n",
              "      <td>229229</td>\n",
              "      <td>34.06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>TAX.csv</td>\n",
              "      <td>67959</td>\n",
              "      <td>36.81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>TEC.csv</td>\n",
              "      <td>14544</td>\n",
              "      <td>19.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>TLA.csv</td>\n",
              "      <td>257192</td>\n",
              "      <td>24.06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>TLI.csv</td>\n",
              "      <td>227875</td>\n",
              "      <td>39.46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>TPN.csv</td>\n",
              "      <td>125726</td>\n",
              "      <td>39.62</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>UAX.csv</td>\n",
              "      <td>152762</td>\n",
              "      <td>28.91</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>UIZ.csv</td>\n",
              "      <td>249299</td>\n",
              "      <td>31.87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>VAL.csv</td>\n",
              "      <td>63297</td>\n",
              "      <td>57.81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>VIF.csv</td>\n",
              "      <td>235974</td>\n",
              "      <td>27.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>XAL.csv</td>\n",
              "      <td>241091</td>\n",
              "      <td>28.17</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   file_name  len_dataset  %datosFaltantes\n",
              "0    ACO.csv       193368            33.19\n",
              "1    AJM.csv       109937            25.45\n",
              "2    AJU.csv        97827            43.39\n",
              "3    ARA.csv        41899            59.76\n",
              "4    ATI.csv       245317            42.49\n",
              "5    AZC.csv        81505            44.70\n",
              "6    BJU.csv       122678            32.91\n",
              "7    CAM.csv       218132            40.68\n",
              "8    CCA.csv       134418            36.57\n",
              "9    CES.csv        85108            31.79\n",
              "10   CHO.csv       200908            38.76\n",
              "11   COY.csv       144137            48.24\n",
              "12   CUA.csv       229608            32.69\n",
              "13   CUT.csv       158321            30.33\n",
              "14   FAC.csv       270737            25.72\n",
              "15   FAR.csv        71260            35.63\n",
              "16   GAM.csv       110172            39.18\n",
              "17   HGM.csv       157736            30.95\n",
              "18   IMP.csv        74286            58.81\n",
              "19   INN.csv       108588            38.54\n",
              "20   IZT.csv       214234            36.63\n",
              "21   LAG.csv        83993            40.69\n",
              "22   LLA.csv       169806            50.28\n",
              "23   LPR.csv       207427            52.99\n",
              "24   MER.csv       257049            22.64\n",
              "25   MGH.csv       117181            26.87\n",
              "26   MON.csv       222329            36.41\n",
              "27   MPA.csv        94460            38.07\n",
              "28   NEZ.csv       195809            32.99\n",
              "29   PED.csv       255672            25.76\n",
              "30   PLA.csv        82556            28.97\n",
              "31   SAC.csv        57084            29.37\n",
              "32   SAG.csv       248463            25.40\n",
              "33   SFE.csv       134231            26.97\n",
              "34   SJA.csv       124413            45.88\n",
              "35   SUR.csv       154987            30.09\n",
              "36   TAC.csv        76250            29.80\n",
              "37   TAH.csv       229229            34.06\n",
              "38   TAX.csv        67959            36.81\n",
              "39   TEC.csv        14544            19.80\n",
              "40   TLA.csv       257192            24.06\n",
              "41   TLI.csv       227875            39.46\n",
              "42   TPN.csv       125726            39.62\n",
              "43   UAX.csv       152762            28.91\n",
              "44   UIZ.csv       249299            31.87\n",
              "45   VAL.csv        63297            57.81\n",
              "46   VIF.csv       235974            27.50\n",
              "47   XAL.csv       241091            28.17"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "info_name = \"info_split_datasets.csv\"\n",
        "info_data = pd.read_csv(info_name)\n",
        "info_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Luego los segmentamos en entrenamiento y prueba (se dejaron para prueba los datos de hace un año)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dir = \"entrenamiento_datos_por_estacion\"\n",
        "dir_sinNaN = \"entrenamiento_datos_por_estacion_sin_NaN\"\n",
        "if not os.path.exists(dir_sinNaN ):\n",
        "    os.makedirs(dir_sinNaN )\n",
        "files = os.listdir(dir)\n",
        "for file_name in files:\n",
        "  file_path = os.path.join(dir,file_name)\n",
        "  print(file_path)\n",
        "  dataset = pd.read_csv(file_path)\n",
        "  dataset = dataset.dropna(axis=1, how='all')\n",
        "  dataset = dataset.dropna(axis=0)\n",
        "  df = dataset[[\"date\", \"year\", \"month\", \"day\", \"hour\", \"minute\"]]\n",
        "  dataset = dataset.drop([\"date\", \"year\", \"month\", \"day\", \"hour\", \"minute\"], axis=1)\n",
        "  for column in dataset.columns:\n",
        "    if dataset[column].nunique() == 1:\n",
        "      dataset = dataset.drop(columns=[column], axis=1)\n",
        "  dataset.insert(0, \"date\", df[\"date\"])\n",
        "  df = df.drop([\"date\"], axis=1)\n",
        "  dataset[[\"year\", \"month\", \"day\", \"hour\", \"minute\"]] = df\n",
        "  name = os.path.join(dir_sinNaN, file_name)\n",
        "  dataset.to_csv(name, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gAd2eJzXKFJ",
        "outputId": "47028060-e615-4754-a0b1-2778159de157"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "datos_por_estacion_sin_NaN\\BJU.csv\n",
            "datos_por_estacion_sin_NaN\\FAC.csv\n",
            "datos_por_estacion_sin_NaN\\NEZ.csv\n",
            "datos_por_estacion_sin_NaN\\PED.csv\n",
            "datos_por_estacion_sin_NaN\\SAC.csv\n"
          ]
        }
      ],
      "source": [
        "dir = \"datos_por_estacion_sin_NaN\"\n",
        "new_dirTest = \"prueba_datos_por_estacion\"\n",
        "new_dirTrain = \"entrenamiento_datos_por_estacion\"\n",
        "\n",
        "if not os.path.exists(new_dirTest):\n",
        "    os.makedirs(new_dirTest)\n",
        "    \n",
        "if not os.path.exists(new_dirTrain):\n",
        "    os.makedirs(new_dirTrain)\n",
        "\n",
        "info_datasets = []\n",
        "columnas = [\"len_dataset_withoutNaN\",\"len_trainSet_withoutNaN\",\"len_testSet_withoutNaN\"]\n",
        "files = os.listdir(dir)\n",
        "for file_name in files:\n",
        "    file_path = os.path.join(dir,file_name)\n",
        "    print(file_path)\n",
        "    dataset = pd.read_csv(file_path)\n",
        "    if len(dataset)>0:\n",
        "        dataset[['year', 'month', 'day']] = dataset['date'].str.split('/', expand=True)\n",
        "        dataset[\"hour\"] = dataset[\"day\"]\n",
        "        dataset[[\"day\", \"hour\"]] = dataset[\"day\"].str.split(' ', expand=True)\n",
        "        dataset[[\"hour\",\"minute\"]] = dataset[\"hour\"].str.split(':', expand=True)\n",
        "        #Ver si se quita o no\n",
        "        #dataset['date'] = pd.to_datetime(dataset['date'], format='%Y/%m/%d %H:%M:%S')\n",
        "        dataset['date'] = pd.to_datetime(dataset['date'])\n",
        "        #dataset['date'] = pd.to_datetime(dataset[['year', 'month', 'day', 'hour','minute']])\n",
        "        #dataset['date'] = pd.to_datetime(dataset['date'])\n",
        "        fecha_limite = pd.Timestamp('2023-04-01')\n",
        "        train_set = dataset[dataset['date'] < fecha_limite].copy()\n",
        "        test_set = dataset[dataset['date'] > fecha_limite].copy()\n",
        "\n",
        "        new_nameTrain = os.path.join(new_dirTrain, file_name)\n",
        "        new_nameTest = os.path.join(new_dirTest, file_name)\n",
        "        train_set.to_csv(new_nameTrain, index=False)\n",
        "        test_set.to_csv(new_nameTest, index=False)\n",
        "        \n",
        "        info_datasets.append([len(dataset), len(train_set), len(test_set)])\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>CO</th>\n",
              "      <th>NO</th>\n",
              "      <th>NOX</th>\n",
              "      <th>NO2</th>\n",
              "      <th>O3</th>\n",
              "      <th>PM25</th>\n",
              "      <th>RH</th>\n",
              "      <th>SO2</th>\n",
              "      <th>TMP</th>\n",
              "      <th>WDR</th>\n",
              "      <th>WSP</th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>day</th>\n",
              "      <th>hour</th>\n",
              "      <th>minute</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2021-09-02 00:00:00</td>\n",
              "      <td>0.52</td>\n",
              "      <td>13.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>97.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>0.7</td>\n",
              "      <td>2021</td>\n",
              "      <td>09</td>\n",
              "      <td>02</td>\n",
              "      <td>00</td>\n",
              "      <td>00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2021-09-02 01:00:00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>95.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>2021</td>\n",
              "      <td>09</td>\n",
              "      <td>02</td>\n",
              "      <td>01</td>\n",
              "      <td>00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2021-09-02 02:00:00</td>\n",
              "      <td>0.16</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>96.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>104.0</td>\n",
              "      <td>2.8</td>\n",
              "      <td>2021</td>\n",
              "      <td>09</td>\n",
              "      <td>02</td>\n",
              "      <td>02</td>\n",
              "      <td>00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2021-09-02 04:00:00</td>\n",
              "      <td>0.15</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>97.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>122.0</td>\n",
              "      <td>1.9</td>\n",
              "      <td>2021</td>\n",
              "      <td>09</td>\n",
              "      <td>02</td>\n",
              "      <td>04</td>\n",
              "      <td>00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2021-09-02 05:00:00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>2.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>96.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>228.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>2021</td>\n",
              "      <td>09</td>\n",
              "      <td>02</td>\n",
              "      <td>05</td>\n",
              "      <td>00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6616</th>\n",
              "      <td>2024-05-15 12:00:00</td>\n",
              "      <td>0.55</td>\n",
              "      <td>1.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>54.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>2.1</td>\n",
              "      <td>2024</td>\n",
              "      <td>05</td>\n",
              "      <td>15</td>\n",
              "      <td>12</td>\n",
              "      <td>00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6617</th>\n",
              "      <td>2024-05-15 13:00:00</td>\n",
              "      <td>0.59</td>\n",
              "      <td>2.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>85.0</td>\n",
              "      <td>48.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>2024</td>\n",
              "      <td>05</td>\n",
              "      <td>15</td>\n",
              "      <td>13</td>\n",
              "      <td>00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6618</th>\n",
              "      <td>2024-05-15 14:00:00</td>\n",
              "      <td>0.56</td>\n",
              "      <td>1.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>81.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>121.0</td>\n",
              "      <td>3.9</td>\n",
              "      <td>2024</td>\n",
              "      <td>05</td>\n",
              "      <td>15</td>\n",
              "      <td>14</td>\n",
              "      <td>00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6619</th>\n",
              "      <td>2024-05-15 15:00:00</td>\n",
              "      <td>0.53</td>\n",
              "      <td>2.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>73.0</td>\n",
              "      <td>61.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>116.0</td>\n",
              "      <td>4.2</td>\n",
              "      <td>2024</td>\n",
              "      <td>05</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6620</th>\n",
              "      <td>2024-05-15 17:00:00</td>\n",
              "      <td>0.80</td>\n",
              "      <td>1.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>79.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>28.0</td>\n",
              "      <td>208.0</td>\n",
              "      <td>4.5</td>\n",
              "      <td>2024</td>\n",
              "      <td>05</td>\n",
              "      <td>15</td>\n",
              "      <td>17</td>\n",
              "      <td>00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6621 rows × 17 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                    date    CO    NO   NOX   NO2    O3  PM25    RH  SO2   TMP  \\\n",
              "0    2021-09-02 00:00:00  0.52  13.0  50.0  37.0   3.0  10.0  97.0  0.0  11.0   \n",
              "1    2021-09-02 01:00:00  0.25   1.0  10.0  10.0  32.0  19.0  95.0  0.0  11.0   \n",
              "2    2021-09-02 02:00:00  0.16   1.0   6.0   5.0  21.0  14.0  96.0  0.0  10.0   \n",
              "3    2021-09-02 04:00:00  0.15   1.0   4.0   4.0  18.0   3.0  97.0  1.0  10.0   \n",
              "4    2021-09-02 05:00:00  0.25   2.0  11.0   9.0  15.0   4.0  96.0  1.0  10.0   \n",
              "...                  ...   ...   ...   ...   ...   ...   ...   ...  ...   ...   \n",
              "6616 2024-05-15 12:00:00  0.55   1.0   8.0   7.0  72.0  54.0  20.0  1.0  29.0   \n",
              "6617 2024-05-15 13:00:00  0.59   2.0  11.0   9.0  85.0  48.0  18.0  1.0  30.0   \n",
              "6618 2024-05-15 14:00:00  0.56   1.0   8.0   6.0  81.0  50.0  19.0  1.0  30.0   \n",
              "6619 2024-05-15 15:00:00  0.53   2.0   7.0   5.0  73.0  61.0  18.0  1.0  30.0   \n",
              "6620 2024-05-15 17:00:00  0.80   1.0  13.0  12.0  79.0  68.0  25.0  1.0  28.0   \n",
              "\n",
              "        WDR  WSP  year month day hour minute  \n",
              "0      40.0  0.7  2021    09  02   00     00  \n",
              "1      29.0  1.5  2021    09  02   01     00  \n",
              "2     104.0  2.8  2021    09  02   02     00  \n",
              "3     122.0  1.9  2021    09  02   04     00  \n",
              "4     228.0  0.8  2021    09  02   05     00  \n",
              "...     ...  ...   ...   ...  ..  ...    ...  \n",
              "6616   80.0  2.1  2024    05  15   12     00  \n",
              "6617   10.0  2.5  2024    05  15   13     00  \n",
              "6618  121.0  3.9  2024    05  15   14     00  \n",
              "6619  116.0  4.2  2024    05  15   15     00  \n",
              "6620  208.0  4.5  2024    05  15   17     00  \n",
              "\n",
              "[6621 rows x 17 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0      2021-09-02 00:00:00\n",
              "1      2021-09-02 01:00:00\n",
              "2      2021-09-02 02:00:00\n",
              "3      2021-09-02 04:00:00\n",
              "4      2021-09-02 05:00:00\n",
              "               ...        \n",
              "6616   2024-05-15 12:00:00\n",
              "6617   2024-05-15 13:00:00\n",
              "6618   2024-05-15 14:00:00\n",
              "6619   2024-05-15 15:00:00\n",
              "6620   2024-05-15 17:00:00\n",
              "Name: date, Length: 6621, dtype: datetime64[ns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.to_datetime(dataset['date'], format='%d/%m/%Y %H:%M')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'datos_por_estacion_sin_NaN\\\\SAC.csv'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "file_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "      <th>len_dataset</th>\n",
              "      <th>%datosFaltantes</th>\n",
              "      <th>len_dataset_withoutNaN</th>\n",
              "      <th>len_trainSet_withoutNaN</th>\n",
              "      <th>len_testSet_withoutNaN</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>BJU.csv</td>\n",
              "      <td>122678</td>\n",
              "      <td>32.91</td>\n",
              "      <td>4197.0</td>\n",
              "      <td>4197.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>FAC.csv</td>\n",
              "      <td>270737</td>\n",
              "      <td>25.72</td>\n",
              "      <td>13123.0</td>\n",
              "      <td>9410.0</td>\n",
              "      <td>3712.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NEZ.csv</td>\n",
              "      <td>195809</td>\n",
              "      <td>32.99</td>\n",
              "      <td>51209.0</td>\n",
              "      <td>46072.0</td>\n",
              "      <td>5137.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>PED.csv</td>\n",
              "      <td>255672</td>\n",
              "      <td>25.76</td>\n",
              "      <td>6621.0</td>\n",
              "      <td>700.0</td>\n",
              "      <td>5921.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SAC.csv</td>\n",
              "      <td>57084</td>\n",
              "      <td>29.37</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  file_name  len_dataset  %datosFaltantes  len_dataset_withoutNaN  \\\n",
              "0   BJU.csv       122678            32.91                  4197.0   \n",
              "1   FAC.csv       270737            25.72                 13123.0   \n",
              "2   NEZ.csv       195809            32.99                 51209.0   \n",
              "3   PED.csv       255672            25.76                  6621.0   \n",
              "4   SAC.csv        57084            29.37                     NaN   \n",
              "\n",
              "   len_trainSet_withoutNaN  len_testSet_withoutNaN  \n",
              "0                   4197.0                     0.0  \n",
              "1                   9410.0                  3712.0  \n",
              "2                  46072.0                  5137.0  \n",
              "3                    700.0                  5921.0  \n",
              "4                      NaN                     NaN  "
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "info_name = \"info_split_datasets.csv\"\n",
        "info_data = pd.read_csv(info_name)\n",
        "info_datasets = pd.DataFrame(info_datasets, columns=columnas)\n",
        "info_data = pd.concat([info_data,info_datasets], axis=1)\n",
        "#info_datasets.to_csv(info_name, index=False)\n",
        "info_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Las carpetas con los datos x estación ya segmentados se encuentran en entrenamiento / prueba / y validación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Esta debería ir antes que la Section 1\n",
        "Section 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se crean nuevas carpetas con archivos sin datos faltantes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "entrenamiento_datos_por_estacion\\AJM.csv\n",
            "entrenamiento_datos_por_estacion\\BJU.csv\n",
            "entrenamiento_datos_por_estacion\\CAM.csv\n",
            "entrenamiento_datos_por_estacion\\CCA.csv\n",
            "entrenamiento_datos_por_estacion\\CUA.csv\n",
            "entrenamiento_datos_por_estacion\\GAM.csv\n",
            "entrenamiento_datos_por_estacion\\MGH.csv\n",
            "entrenamiento_datos_por_estacion\\NEZ.csv\n",
            "entrenamiento_datos_por_estacion\\PED.csv\n",
            "entrenamiento_datos_por_estacion\\SAC.csv\n",
            "entrenamiento_datos_por_estacion\\SAG.csv\n",
            "entrenamiento_datos_por_estacion\\SFE.csv\n",
            "entrenamiento_datos_por_estacion\\TAH.csv\n",
            "entrenamiento_datos_por_estacion\\TLI.csv\n",
            "entrenamiento_datos_por_estacion\\VIF.csv\n",
            "validacion_datos_por_estacion\\AJM.csv\n",
            "validacion_datos_por_estacion\\BJU.csv\n",
            "validacion_datos_por_estacion\\CAM.csv\n",
            "validacion_datos_por_estacion\\CCA.csv\n",
            "validacion_datos_por_estacion\\CUA.csv\n",
            "validacion_datos_por_estacion\\GAM.csv\n",
            "validacion_datos_por_estacion\\MGH.csv\n",
            "validacion_datos_por_estacion\\NEZ.csv\n",
            "validacion_datos_por_estacion\\PED.csv\n",
            "validacion_datos_por_estacion\\SAC.csv\n",
            "validacion_datos_por_estacion\\SAG.csv\n",
            "validacion_datos_por_estacion\\SFE.csv\n",
            "validacion_datos_por_estacion\\TAH.csv\n",
            "validacion_datos_por_estacion\\TLI.csv\n",
            "validacion_datos_por_estacion\\VIF.csv\n",
            "prueba_datos_por_estacion\\AJM.csv\n",
            "prueba_datos_por_estacion\\BJU.csv\n",
            "prueba_datos_por_estacion\\CAM.csv\n",
            "prueba_datos_por_estacion\\CCA.csv\n",
            "prueba_datos_por_estacion\\CUA.csv\n",
            "prueba_datos_por_estacion\\GAM.csv\n",
            "prueba_datos_por_estacion\\MGH.csv\n",
            "prueba_datos_por_estacion\\NEZ.csv\n",
            "prueba_datos_por_estacion\\PED.csv\n",
            "prueba_datos_por_estacion\\SAC.csv\n",
            "prueba_datos_por_estacion\\SAG.csv\n",
            "prueba_datos_por_estacion\\SFE.csv\n",
            "prueba_datos_por_estacion\\TAH.csv\n",
            "prueba_datos_por_estacion\\TLI.csv\n",
            "prueba_datos_por_estacion\\VIF.csv\n"
          ]
        }
      ],
      "source": [
        "dir = \"entrenamiento_datos_por_estacion\"\n",
        "dir_sinNaN = \"entrenamiento_datos_por_estacion_sin_NaN\"\n",
        "if not os.path.exists(dir_sinNaN ):\n",
        "    os.makedirs(dir_sinNaN )\n",
        "files = os.listdir(dir)\n",
        "for file_name in files:\n",
        "  file_path = os.path.join(dir,file_name)\n",
        "  print(file_path)\n",
        "  dataset = pd.read_csv(file_path)\n",
        "  dataset = dataset.dropna(axis=1, how='all')\n",
        "  dataset = dataset.dropna(axis=0)\n",
        "  df = dataset[[\"date\", \"year\", \"month\", \"day\", \"hour\", \"minute\"]]\n",
        "  dataset = dataset.drop([\"date\", \"year\", \"month\", \"day\", \"hour\", \"minute\"], axis=1)\n",
        "  for column in dataset.columns:\n",
        "    if dataset[column].nunique() == 1:\n",
        "      dataset = dataset.drop(columns=[column], axis=1)\n",
        "  dataset.insert(0, \"date\", df[\"date\"])\n",
        "  df = df.drop([\"date\"], axis=1)\n",
        "  dataset[[\"year\", \"month\", \"day\", \"hour\", \"minute\"]] = df\n",
        "  name = os.path.join(dir_sinNaN, file_name)\n",
        "  dataset.to_csv(name, index=False)\n",
        "\n",
        "dir = \"validacion_datos_por_estacion\"\n",
        "dir_sinNaN = \"validacion_datos_por_estacion_sin_NaN\"\n",
        "if not os.path.exists(dir_sinNaN):\n",
        "    os.makedirs(dir_sinNaN )\n",
        "files = os.listdir(dir)\n",
        "for file_name in files:\n",
        "  file_path = os.path.join(dir,file_name)\n",
        "  print(file_path)\n",
        "  dataset = pd.read_csv(file_path)\n",
        "  dataset = dataset.dropna(axis=1, how='all')\n",
        "  dataset = dataset.dropna(axis=0)\n",
        "  df = dataset[[\"date\", \"year\", \"month\", \"day\", \"hour\", \"minute\"]]\n",
        "  dataset = dataset.drop([\"date\", \"year\", \"month\", \"day\", \"hour\", \"minute\"], axis=1)\n",
        "  for column in dataset.columns:\n",
        "    if dataset[column].nunique() == 1:\n",
        "      dataset = dataset.drop(columns=[column], axis=1)\n",
        "  dataset.insert(0, \"date\", df[\"date\"])\n",
        "  df = df.drop([\"date\"], axis=1)\n",
        "  dataset[[\"year\", \"month\", \"day\", \"hour\", \"minute\"]] = df\n",
        "  name = os.path.join(dir_sinNaN, file_name)\n",
        "  dataset.to_csv(name, index=False)\n",
        "\n",
        "\n",
        "dir = \"prueba_datos_por_estacion\"\n",
        "dir_sinNaN = \"prueba_datos_por_estacion_sin_NaN\"\n",
        "if not os.path.exists(dir_sinNaN):\n",
        "    os.makedirs(dir_sinNaN )\n",
        "files = os.listdir(dir)\n",
        "for file_name in files:\n",
        "  file_path = os.path.join(dir,file_name)\n",
        "  print(file_path)\n",
        "  dataset = pd.read_csv(file_path)\n",
        "  dataset = dataset.dropna(axis=1, how='all')\n",
        "  dataset = dataset.dropna(axis=0)\n",
        "  df = dataset[[\"date\", \"year\", \"month\", \"day\", \"hour\", \"minute\"]]\n",
        "  dataset = dataset.drop([\"date\", \"year\", \"month\", \"day\", \"hour\", \"minute\"], axis=1)\n",
        "  for column in dataset.columns:\n",
        "    if dataset[column].nunique() == 1:\n",
        "      dataset = dataset.drop(columns=[column], axis=1)\n",
        "  dataset.insert(0, \"date\", df[\"date\"])\n",
        "  df = df.drop([\"date\"], axis=1)\n",
        "  dataset[[\"year\", \"month\", \"day\", \"hour\", \"minute\"]] = df\n",
        "  name = os.path.join(dir_sinNaN, file_name)\n",
        "  dataset.to_csv(name, index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
